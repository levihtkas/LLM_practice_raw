{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf7e4c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97932953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wanj\n"
     ]
    }
   ],
   "source": [
    "print(\"wanj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06c5c1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import requests \n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display,Markdown\n",
    "load_dotenv(override=True)\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9c2af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "anthropic_url = \"https://api.anthropic.com/v1/\"\n",
    "gemini_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "\n",
    "openai = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "gemini = OpenAI(api_key=google_api_key,base_url=gemini_url)\n",
    "\n",
    "claude = OpenAI(api_key=anthropic_api_key,base_url=anthropic_url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1595a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{'role':'user','content':'Tell me a joke about LLM engineering'},{'role':'system','content':'You have to make them learn LLM'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d38e6710",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.chat.completions.create(model='gpt-4.1-mini',messages=messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce6f9e60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Why did the LLM engineer bring a ladder to the training session?\n",
       "\n",
       "Because they wanted to help the model *learn* on a whole new level!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b324f65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = claude.chat.completions.create(model='claude-sonnet-4-5-20250929',messages=messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89689f7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here's one for you:\n",
       "\n",
       "Why did the LLM engineer break up with their model?\n",
       "\n",
       "Because it kept hallucinating about their relationship! \n",
       "\n",
       "Every time they asked \"Do you love me?\" it would confidently respond with a different answer‚Äîsometimes citing sources that didn't exist, occasionally claiming they were married with three kids, and once it even generated an entire backstory about how they met at a conference that never happened.\n",
       "\n",
       "The final straw? When the engineer asked for space, the model said \"Sure, I have 128k tokens of context window‚Äîplenty of space!\" \n",
       "\n",
       "---\n",
       "\n",
       "*Bonus groaner*: What's an LLM engineer's favorite type of music? \n",
       "\n",
       "Heavy **token metal**! üé∏\n",
       "\n",
       "(And they always complain about the **latency** in the beat! ü•Å)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb608680",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{'role':'user','content':'Tell me a joke about LLM engineering'},{'role':'system','content':'You have to make them learn LLM in JSON'}]\n",
    "\n",
    "res = openai.chat.completions.create(model=\"gpt-4.1-nano\",messages=messages,\n",
    "response_format={ \"type\": \"json_object\" }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a7b4f491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "{\n",
       "  \"joke\": {\n",
       "    \"question\": \"Why did the LLM engineer bring a JSON file to the party?\",\n",
       "    \"answer\": \"Because they wanted to make sure everyone was properly formatted and easily parsable!\"\n",
       "  }\n",
       "}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(res.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f2ede2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inferences Time Vs Training Time\n",
    "# reasoning effort ssays about trade off between Inferences time and training time \n",
    "\n",
    "## Lets take puzzles to make it fun \n",
    "easy_puzzle = [\n",
    "    {\"role\": \"user\", \"content\": \n",
    "        \"You toss 2 coins. One of them is heads. What's the probability the other is tails? Answer with the probability only.\"},\n",
    "]\n",
    "\n",
    "hard = \"\"\"\n",
    "On a bookshelf, two volumes of Pushkin stand side by side: the first and the second.\n",
    "The pages of each volume together have a thickness of 2 cm, and each cover is 2 mm thick.\n",
    "A worm gnawed (perpendicular to the pages) from the first page of the first volume to the last page of the second volume.\n",
    "What distance did it gnaw through?\n",
    "\"\"\"\n",
    "\n",
    "hard_puzzle = [\n",
    "    {'role':'user','content':hard}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "94d5acb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "4 mm.\n",
       "\n",
       "Explanation: When the two volumes stand in order (1 left, 2 right), the first page of vol.1 lies immediately inside its front cover (the right face of vol.1) and the last page of vol.2 lies immediately inside its back cover (the left face of vol.2). Those two covers touch, so the worm only chewed through the two facing covers: 2 mm + 2 mm = 4 mm (0.4 cm)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now with reasoning_effort we can limit the resposnse time of an LLM therby making it answer quickly \n",
    "response = openai.chat.completions.create(model = 'gpt-5-mini',messages=hard_puzzle,reasoning_effort=\"medium\")\n",
    "\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "526d92d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Ollama is running'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "requests.get(\"http://localhost:11434/\").content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4fff53eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0535e520",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama = OpenAI(base_url = \"http://localhost:11434/v1\",api_key='des')\n",
    "# response = ollama.chat.completions.create(model='llama3.2',messages=easy_puzzle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c0fe8c24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "1/2"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = ollama.chat.completions.create(model=\"llama3.2\", messages=easy_puzzle)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "13d5d1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ollama.chat.completions.create(model='llama3.2',messages=easy_puzzle,response_format= {'type':'json_object'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "48b1d66c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "{  }"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6ae8f69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.12.6 environment at: c:\\Users\\vijib\\projects\\llm_eng_manual\\.venv\u001b[0m\n",
      "\u001b[2mResolved \u001b[1m23 packages\u001b[0m \u001b[2min 2.25s\u001b[0m\u001b[0m\n",
      "\u001b[2mPrepared \u001b[1m3 packages\u001b[0m \u001b[2min 605ms\u001b[0m\u001b[0m\n",
      "\u001b[2mInstalled \u001b[1m8 packages\u001b[0m \u001b[2min 951ms\u001b[0m\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcachetools\u001b[0m\u001b[2m==6.2.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgoogle-auth\u001b[0m\u001b[2m==2.43.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgoogle-genai\u001b[0m\u001b[2m==1.52.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyasn1\u001b[0m\u001b[2m==0.6.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyasn1-modules\u001b[0m\u001b[2m==0.4.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrsa\u001b[0m\u001b[2m==4.9.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtenacity\u001b[0m\u001b[2m==9.1.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mwebsockets\u001b[0m\u001b[2m==15.0.1\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install google-genai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df949620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamil Nadu is a vibrant state in South India that offers a truly enriching experience, and it's understandable to have concerns about hygiene and safety when traveling to a new country. However, I can assure you that Tamil Nadu, like many parts of India, is rapidly evolving and offers many places and experiences that prioritize cleanliness and security.\n",
      "\n",
      "Let's paint a picture of Tamil Nadu, focusing on aspects that might address your concerns:\n",
      "\n",
      "**Focusing on Hygiene:**\n",
      "\n",
      "*   **Modern Infrastructure & Growing Awareness:** Major cities like Chennai and Coimbatore have seen significant development. You'll find modern hotels, restaurants, and shopping malls that adhere to international standards of cleanliness. The awareness about hygiene is growing rapidly across the state, with many establishments taking proactive measures.\n",
      "*   **Cleanliness in Tourist Spots:** Many popular tourist destinations are well-maintained. Think of the UNESCO World Heritage sites like the **Group of Monuments at Mahabalipuram**, where efforts are made to keep the surroundings clean. Beautiful beaches like **Marina Beach** in Chennai are actively cleaned, and in quieter, less crowded areas, you can find pristine stretches of sand.\n",
      "*   **Guesthouses and Boutique Stays:** Beyond large hotels, there are charming guesthouses and boutique homestays that often offer a more intimate and meticulously maintained environment. Many are run by families who take immense pride in their hospitality and cleanliness.\n",
      "*   **Focus on Fresh, Healthy Food:** Tamil Nadu is renowned for its delicious South Indian cuisine. You'll find plenty of options for fresh, cooked-to-order meals. Many restaurants, especially those catering to tourists, are careful about food preparation and sourcing. Eating at reputable establishments, looking for busy places with good turnover (a sign of freshness), and choosing freshly cooked dishes will ensure a pleasant culinary experience.\n",
      "*   **Water Safety:** Most hotels and guesthouses provide bottled water, and it's widely available everywhere. It's always advisable to stick to bottled or purified water for drinking.\n",
      "\n",
      "**Focusing on Safety:**\n",
      "\n",
      "*   **Welcoming Culture:** Tamilians are known for their warmth, hospitality, and a generally respectful attitude towards visitors. You'll often find people eager to help if you need directions or assistance.\n",
      "*   **Tourist Police & Security:** Major tourist areas and popular attractions often have a visible presence of tourist police or security personnel, ensuring a safe environment for visitors.\n",
      "*   **Well-Connected and Accessible:** Tamil Nadu has excellent transportation networks. Major cities are well-connected by air, rail, and road. Within cities, you have access to taxis, ride-sharing apps (like Ola and Uber), and auto-rickshaws, all of which offer relatively safe and convenient ways to get around.\n",
      "*   **Vibrant Cities with Modern Amenities:** Chennai, the capital, is a bustling metropolis with a modern outlook. It's a hub for business, education, and healthcare, with a well-developed infrastructure that supports safety and security.\n",
      "*   **Spiritual and Cultural Sanctuaries:** Places like the magnificent temples of **Madurai (Meenakshi Temple)**, **Thanjavur (Brihadeeswarar Temple)**, and **Rameswaram (Ramanathaswamy Temple)** are not only awe-inspiring but are also generally very safe and orderly environments, often with security measures in place.\n",
      "*   **Escapades into Nature:** For a more serene experience, consider hill stations like **Ooty** or **Kodaikanal**. These offer cooler climates, lush green landscapes, and a generally tranquil atmosphere where safety is rarely a concern for tourists.\n",
      "*   **Responsible Tourism Initiatives:** The state government and various organizations are increasingly promoting responsible tourism, which includes ensuring the well-being and safety of visitors.\n",
      "\n",
      "**What to Expect and How to Prepare:**\n",
      "\n",
      "*   **Research is Key:** As with any travel, doing your research on specific areas and accommodations you plan to visit is always a good idea. Read reviews from other travelers.\n",
      "*   **Trust Your Instincts:** If a situation feels uncomfortable, it's okay to remove yourself from it.\n",
      "*   **Be Aware of Your Surroundings:** This is good advice for any traveler anywhere in the world.\n",
      "*   **Embrace the Experience:** While it's important to be prepared, try not to let anxieties overshadow the incredible cultural richness, stunning architecture, delicious food, and warm people that Tamil Nadu has to offer.\n",
      "\n",
      "Tamil Nadu is a state that offers a deeply rewarding travel experience. By choosing reputable establishments, being mindful of your surroundings, and embracing the local culture with an open mind, you'll likely find it to be a safe and wonderfully clean destination. It's a place where ancient traditions meet modern aspirations, offering a unique and unforgettable journey.\n"
     ]
    }
   ],
   "source": [
    "# We are primarly using the openai API key now for context purpose look at some \n",
    "\n",
    "from google import genai \n",
    "\n",
    "client = genai.Client()\n",
    "#genai its models.generate_content\n",
    "response = client.models.generate_content(\n",
    "  model = 'gemini-2.5-flash-lite',\n",
    "  contents = \"Describe Tamil Nadu to someone who thinks India is unhygenic and not safe\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6a035b5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Tamil Nadu is a vibrant state in South India that offers a truly enriching experience, and it's understandable to have concerns about hygiene and safety when traveling to a new country. However, I can assure you that Tamil Nadu, like many parts of India, is rapidly evolving and offers many places and experiences that prioritize cleanliness and security.\n",
       "\n",
       "Let's paint a picture of Tamil Nadu, focusing on aspects that might address your concerns:\n",
       "\n",
       "**Focusing on Hygiene:**\n",
       "\n",
       "*   **Modern Infrastructure & Growing Awareness:** Major cities like Chennai and Coimbatore have seen significant development. You'll find modern hotels, restaurants, and shopping malls that adhere to international standards of cleanliness. The awareness about hygiene is growing rapidly across the state, with many establishments taking proactive measures.\n",
       "*   **Cleanliness in Tourist Spots:** Many popular tourist destinations are well-maintained. Think of the UNESCO World Heritage sites like the **Group of Monuments at Mahabalipuram**, where efforts are made to keep the surroundings clean. Beautiful beaches like **Marina Beach** in Chennai are actively cleaned, and in quieter, less crowded areas, you can find pristine stretches of sand.\n",
       "*   **Guesthouses and Boutique Stays:** Beyond large hotels, there are charming guesthouses and boutique homestays that often offer a more intimate and meticulously maintained environment. Many are run by families who take immense pride in their hospitality and cleanliness.\n",
       "*   **Focus on Fresh, Healthy Food:** Tamil Nadu is renowned for its delicious South Indian cuisine. You'll find plenty of options for fresh, cooked-to-order meals. Many restaurants, especially those catering to tourists, are careful about food preparation and sourcing. Eating at reputable establishments, looking for busy places with good turnover (a sign of freshness), and choosing freshly cooked dishes will ensure a pleasant culinary experience.\n",
       "*   **Water Safety:** Most hotels and guesthouses provide bottled water, and it's widely available everywhere. It's always advisable to stick to bottled or purified water for drinking.\n",
       "\n",
       "**Focusing on Safety:**\n",
       "\n",
       "*   **Welcoming Culture:** Tamilians are known for their warmth, hospitality, and a generally respectful attitude towards visitors. You'll often find people eager to help if you need directions or assistance.\n",
       "*   **Tourist Police & Security:** Major tourist areas and popular attractions often have a visible presence of tourist police or security personnel, ensuring a safe environment for visitors.\n",
       "*   **Well-Connected and Accessible:** Tamil Nadu has excellent transportation networks. Major cities are well-connected by air, rail, and road. Within cities, you have access to taxis, ride-sharing apps (like Ola and Uber), and auto-rickshaws, all of which offer relatively safe and convenient ways to get around.\n",
       "*   **Vibrant Cities with Modern Amenities:** Chennai, the capital, is a bustling metropolis with a modern outlook. It's a hub for business, education, and healthcare, with a well-developed infrastructure that supports safety and security.\n",
       "*   **Spiritual and Cultural Sanctuaries:** Places like the magnificent temples of **Madurai (Meenakshi Temple)**, **Thanjavur (Brihadeeswarar Temple)**, and **Rameswaram (Ramanathaswamy Temple)** are not only awe-inspiring but are also generally very safe and orderly environments, often with security measures in place.\n",
       "*   **Escapades into Nature:** For a more serene experience, consider hill stations like **Ooty** or **Kodaikanal**. These offer cooler climates, lush green landscapes, and a generally tranquil atmosphere where safety is rarely a concern for tourists.\n",
       "*   **Responsible Tourism Initiatives:** The state government and various organizations are increasingly promoting responsible tourism, which includes ensuring the well-being and safety of visitors.\n",
       "\n",
       "**What to Expect and How to Prepare:**\n",
       "\n",
       "*   **Research is Key:** As with any travel, doing your research on specific areas and accommodations you plan to visit is always a good idea. Read reviews from other travelers.\n",
       "*   **Trust Your Instincts:** If a situation feels uncomfortable, it's okay to remove yourself from it.\n",
       "*   **Be Aware of Your Surroundings:** This is good advice for any traveler anywhere in the world.\n",
       "*   **Embrace the Experience:** While it's important to be prepared, try not to let anxieties overshadow the incredible cultural richness, stunning architecture, delicious food, and warm people that Tamil Nadu has to offer.\n",
       "\n",
       "Tamil Nadu is a state that offers a deeply rewarding travel experience. By choosing reputable establishments, being mindful of your surroundings, and embracing the local culture with an open mind, you'll likely find it to be a safe and wonderfully clean destination. It's a place where ancient traditions meet modern aspirations, offering a unique and unforgettable journey."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown((response.text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ec26ae8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting anthropic\n",
      "  Downloading anthropic-0.75.0-py3-none-any.whl.metadata (28 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\vijib\\projects\\llm_eng_manual\\.venv\\lib\\site-packages (from anthropic) (4.11.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\vijib\\projects\\llm_eng_manual\\.venv\\lib\\site-packages (from anthropic) (1.9.0)\n",
      "Collecting docstring-parser<1,>=0.15 (from anthropic)\n",
      "  Using cached docstring_parser-0.17.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.25.0 in c:\\users\\vijib\\projects\\llm_eng_manual\\.venv\\lib\\site-packages (from anthropic) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\vijib\\projects\\llm_eng_manual\\.venv\\lib\\site-packages (from anthropic) (0.12.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\vijib\\projects\\llm_eng_manual\\.venv\\lib\\site-packages (from anthropic) (2.12.4)\n",
      "Requirement already satisfied: sniffio in c:\\users\\vijib\\projects\\llm_eng_manual\\.venv\\lib\\site-packages (from anthropic) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in c:\\users\\vijib\\projects\\llm_eng_manual\\.venv\\lib\\site-packages (from anthropic) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\vijib\\projects\\llm_eng_manual\\.venv\\lib\\site-packages (from anyio<5,>=3.5.0->anthropic) (3.11)\n",
      "Requirement already satisfied: certifi in c:\\users\\vijib\\projects\\llm_eng_manual\\.venv\\lib\\site-packages (from httpx<1,>=0.25.0->anthropic) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\vijib\\projects\\llm_eng_manual\\.venv\\lib\\site-packages (from httpx<1,>=0.25.0->anthropic) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\vijib\\projects\\llm_eng_manual\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\vijib\\projects\\llm_eng_manual\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\vijib\\projects\\llm_eng_manual\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->anthropic) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\vijib\\projects\\llm_eng_manual\\.venv\\lib\\site-packages (from pydantic<3,>=1.9.0->anthropic) (0.4.2)\n",
      "Downloading anthropic-0.75.0-py3-none-any.whl (388 kB)\n",
      "Using cached docstring_parser-0.17.0-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: docstring-parser, anthropic\n",
      "\n",
      "   ---------------------------------------- 0/2 [docstring-parser]\n",
      "   ---------------------------------------- 0/2 [docstring-parser]\n",
      "   ---------------------------------------- 0/2 [docstring-parser]\n",
      "   ---------------------------------------- 0/2 [docstring-parser]\n",
      "   ---------------------------------------- 0/2 [docstring-parser]\n",
      "   ---------------------------------------- 0/2 [docstring-parser]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   -------------------- ------------------- 1/2 [anthropic]\n",
      "   ---------------------------------------- 2/2 [anthropic]\n",
      "\n",
      "Successfully installed anthropic-0.75.0 docstring-parser-0.17.0\n"
     ]
    }
   ],
   "source": [
    "!pip install anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1cadc08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import Anthropic\n",
    "\n",
    "client = Anthropic()\n",
    "\n",
    "response = client.messages.create(\n",
    "  model = 'claude-haiku-4-5',\n",
    "  messages=[{\"role\": \"user\", \"content\": \"Describe the color Blue to someone who's never been able to see in 1 sentence\"}],\n",
    "    max_tokens=100\n",
    ")\n",
    "# os.getenv('ANTHROPIC_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f40995",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "42d71d69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Blue is the cool, calm feeling you get from a gentle breeze or refreshing water on your skin‚Äîa color associated with peace, openness, and depth, like the vastness of the sky or ocean."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response.content[0].text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c23088",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'openrouter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[86]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m response = \u001b[43mopenrouter\u001b[49m.chat.completions.create(model=\u001b[33m\"\u001b[39m\u001b[33mz-ai/glm-4.5\u001b[39m\u001b[33m\"\u001b[39m, messages=tell_a_joke)\n\u001b[32m      2\u001b[39m display(Markdown(response.choices[\u001b[32m0\u001b[39m].message.content))\n",
      "\u001b[31mNameError\u001b[39m: name 'openrouter' is not defined"
     ]
    }
   ],
   "source": [
    "response = openrouter.chat.completions.create(model=\"z-ai/glm-4.5\", messages=tell_a_joke)\n",
    "display(Markdown(response.choices[0].message.content))\n",
    "#No API key just for educational purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c65fa990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_openai\n",
      "  Downloading langchain_openai-1.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting langchain-core<2.0.0,>=1.1.0 (from langchain_openai)\n",
      "  Downloading langchain_core-1.1.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: openai<3.0.0,>=1.109.1 in c:\\users\\vijib\\projects\\llm_eng_manual\\.venv\\lib\\site-packages (from langchain_openai) (2.8.1)\n",
      "Collecting tiktoken<1.0.0,>=0.7.0 (from langchain_openai)\n",
      "  Using cached tiktoken-0.12.0-cp312-cp312-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting jsonpatch<2.0.0,>=1.33.0 (from langchain-core<2.0.0,>=1.1.0->langchain_openai)\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langsmith<1.0.0,>=0.3.45 (from langchain-core<2.0.0,>=1.1.0->langchain_openai)\n",
      "  Downloading langsmith-0.4.53-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\vijib\\projects\\llm_eng_manual\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.0->langchain_openai) (25.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\vijib\\projects\\llm_eng_manual\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.0->langchain_openai) (2.12.4)\n",
      "Collecting pyyaml<7.0.0,>=5.3.0 (from langchain-core<2.0.0,>=1.1.0->langchain_openai)\n",
      "  Using cached pyyaml-6.0.3-cp312-cp312-win_amd64.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\vijib\\projects\\llm_eng_manual\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.0->langchain_openai) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in c:\\users\\vijib\\projects\\llm_eng_manual\\.venv\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.0->langchain_openai) (4.15.0)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.1.0->langchain_openai)\n",
      "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\vijib\\projects\\llm_eng_manual\\.venv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain_openai) (0.28.1)\n",
      "Collecting orjson>=3.9.14 (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain_openai)\n",
      "  Using cached orjson-3.11.4-cp312-cp312-win_amd64.whl.metadata (42 kB)\n",
      "Collecting requests-toolbelt>=1.0.0 (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain_openai)\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: requests>=2.0.0 in c:\\users\\vijib\\projects\\llm_eng_manual\\.venv\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain_openai) (2.32.5)\n",
      "Collecting uuid-utils<1.0,>=0.12.0 (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain_openai)\n",
      "  Downloading uuid_utils-0.12.0-cp39-abi3-win_amd64.whl.metadata (1.1 kB)\n",
      "Collecting zstandard>=0.23.0 (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain_openai)\n",
      "  Using cached zstandard-0.25.0-cp312-cp312-win_amd64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: anyio in c:\\users\\vijib\\projects\\llm_eng_manual\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain_openai) (4.11.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\vijib\\projects\\llm_eng_manual\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain_openai) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\vijib\\projects\\llm_eng_manual\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain_openai) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\vijib\\projects\\llm_eng_manual\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain_openai) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\vijib\\projects\\llm_eng_manual\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain_openai) (0.16.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\vijib\\projects\\llm_eng_manual\\.venv\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in c:\\users\\vijib\\projects\\llm_eng_manual\\.venv\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (0.12.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\vijib\\projects\\llm_eng_manual\\.venv\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\vijib\\projects\\llm_eng_manual\\.venv\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain_openai) (4.67.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\vijib\\projects\\llm_eng_manual\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.1.0->langchain_openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\vijib\\projects\\llm_eng_manual\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.1.0->langchain_openai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\vijib\\projects\\llm_eng_manual\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.1.0->langchain_openai) (0.4.2)\n",
      "Collecting regex>=2022.1.18 (from tiktoken<1.0.0,>=0.7.0->langchain_openai)\n",
      "  Using cached regex-2025.11.3-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\vijib\\projects\\llm_eng_manual\\.venv\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain_openai) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vijib\\projects\\llm_eng_manual\\.venv\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain_openai) (2.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\vijib\\projects\\llm_eng_manual\\.venv\\lib\\site-packages (from tqdm>4->openai<3.0.0,>=1.109.1->langchain_openai) (0.4.6)\n",
      "Downloading langchain_openai-1.1.0-py3-none-any.whl (84 kB)\n",
      "Downloading langchain_core-1.1.0-py3-none-any.whl (473 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading langsmith-0.4.53-py3-none-any.whl (411 kB)\n",
      "Using cached pyyaml-6.0.3-cp312-cp312-win_amd64.whl (154 kB)\n",
      "Using cached tiktoken-0.12.0-cp312-cp312-win_amd64.whl (878 kB)\n",
      "Downloading uuid_utils-0.12.0-cp39-abi3-win_amd64.whl (183 kB)\n",
      "Using cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached orjson-3.11.4-cp312-cp312-win_amd64.whl (131 kB)\n",
      "Using cached regex-2025.11.3-cp312-cp312-win_amd64.whl (277 kB)\n",
      "Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Using cached zstandard-0.25.0-cp312-cp312-win_amd64.whl (506 kB)\n",
      "Installing collected packages: zstandard, uuid-utils, regex, pyyaml, orjson, jsonpointer, tiktoken, requests-toolbelt, jsonpatch, langsmith, langchain-core, langchain_openai\n",
      "\n",
      "   ------ ---------------------------------  2/12 [regex]\n",
      "   ------ ---------------------------------  2/12 [regex]\n",
      "   ---------- -----------------------------  3/12 [pyyaml]\n",
      "   ---------- -----------------------------  3/12 [pyyaml]\n",
      "   ---------------- -----------------------  5/12 [jsonpointer]\n",
      "   -------------------- -------------------  6/12 [tiktoken]\n",
      "   ----------------------- ----------------  7/12 [requests-toolbelt]\n",
      "   ----------------------- ----------------  7/12 [requests-toolbelt]\n",
      "   ----------------------- ----------------  7/12 [requests-toolbelt]\n",
      "   ----------------------- ----------------  7/12 [requests-toolbelt]\n",
      "   -------------------------- -------------  8/12 [jsonpatch]\n",
      "   ------------------------------ ---------  9/12 [langsmith]\n",
      "   ------------------------------ ---------  9/12 [langsmith]\n",
      "   ------------------------------ ---------  9/12 [langsmith]\n",
      "   ------------------------------ ---------  9/12 [langsmith]\n",
      "   ------------------------------ ---------  9/12 [langsmith]\n",
      "   ------------------------------ ---------  9/12 [langsmith]\n",
      "   ------------------------------ ---------  9/12 [langsmith]\n",
      "   ------------------------------ ---------  9/12 [langsmith]\n",
      "   ------------------------------ ---------  9/12 [langsmith]\n",
      "   ------------------------------ ---------  9/12 [langsmith]\n",
      "   ------------------------------ ---------  9/12 [langsmith]\n",
      "   ------------------------------ ---------  9/12 [langsmith]\n",
      "   --------------------------------- ------ 10/12 [langchain-core]\n",
      "   --------------------------------- ------ 10/12 [langchain-core]\n",
      "   --------------------------------- ------ 10/12 [langchain-core]\n",
      "   --------------------------------- ------ 10/12 [langchain-core]\n",
      "   --------------------------------- ------ 10/12 [langchain-core]\n",
      "   --------------------------------- ------ 10/12 [langchain-core]\n",
      "   --------------------------------- ------ 10/12 [langchain-core]\n",
      "   --------------------------------- ------ 10/12 [langchain-core]\n",
      "   --------------------------------- ------ 10/12 [langchain-core]\n",
      "   --------------------------------- ------ 10/12 [langchain-core]\n",
      "   --------------------------------- ------ 10/12 [langchain-core]\n",
      "   --------------------------------- ------ 10/12 [langchain-core]\n",
      "   --------------------------------- ------ 10/12 [langchain-core]\n",
      "   --------------------------------- ------ 10/12 [langchain-core]\n",
      "   --------------------------------- ------ 10/12 [langchain-core]\n",
      "   --------------------------------- ------ 10/12 [langchain-core]\n",
      "   --------------------------------- ------ 10/12 [langchain-core]\n",
      "   --------------------------------- ------ 10/12 [langchain-core]\n",
      "   --------------------------------- ------ 10/12 [langchain-core]\n",
      "   --------------------------------- ------ 10/12 [langchain-core]\n",
      "   --------------------------------- ------ 10/12 [langchain-core]\n",
      "   --------------------------------- ------ 10/12 [langchain-core]\n",
      "   --------------------------------- ------ 10/12 [langchain-core]\n",
      "   --------------------------------- ------ 10/12 [langchain-core]\n",
      "   --------------------------------- ------ 10/12 [langchain-core]\n",
      "   ------------------------------------ --- 11/12 [langchain_openai]\n",
      "   ------------------------------------ --- 11/12 [langchain_openai]\n",
      "   ------------------------------------ --- 11/12 [langchain_openai]\n",
      "   ------------------------------------ --- 11/12 [langchain_openai]\n",
      "   ---------------------------------------- 12/12 [langchain_openai]\n",
      "\n",
      "Successfully installed jsonpatch-1.33 jsonpointer-3.0.0 langchain-core-1.1.0 langchain_openai-1.1.0 langsmith-0.4.53 orjson-3.11.4 pyyaml-6.0.3 regex-2025.11.3 requests-toolbelt-1.0.0 tiktoken-0.12.0 uuid-utils-0.12.0 zstandard-0.25.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "314f9c66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Short answer\n",
       "- LLMs have become far more capable, broadly useful, and integrated into real products, but they still make mistakes (hallucinate), can be biased, and need retrieval/instrumentation for reliable real-world use.\n",
       "- Big recent advances: instruction tuning and RLHF for safer conversational behavior; retrieval + tool use for factuality; multimodal inputs; efficiency techniques (quantization, LoRA) that make large models cheaper to run.\n",
       "\n",
       "(I'm summarizing trends up to mid‚Äë2024 ‚Äî many of these trends have continued since.)\n",
       "\n",
       "What‚Äôs improved\n",
       "- Core capabilities: much better at coding, complex reasoning prompts, long-form writing, summarization, and multi-turn dialogue.\n",
       "- Instruction following: models are more consistent and helpful after instruction tuning and reinforcement learning from human feedback (RLHF).\n",
       "- Retrieval & tools: combining LLMs with retrieval-augmented generation (RAG), up-to-date search, calculators, and external tools greatly reduces hallucinations and enables current-information tasks.\n",
       "- Multimodality: models can process images (and in some research, audio/video) along with text, enabling tasks like image-aware Q&A, document understanding, and visual coding help.\n",
       "- Efficiency & deployment: model compression, quantization, sparsity, and adapter/LoRA methods let smaller teams fine-tune and run powerful models cheaper; specialized inference hardware and runtime optimizations improved latency.\n",
       "- Ecosystem: more open-source models and toolchains, plus commercial APIs that integrate safety and tooling.\n",
       "\n",
       "Main limitations & risks\n",
       "- Hallucinations: models still fabricate facts or overconfidently present incorrect answers unless constrained by retrieval or verification.\n",
       "- Reasoning gaps: for very long, multi-step, or novel reasoning tasks, errors still occur.\n",
       "- Safety/bias: toxic, biased, or otherwise harmful outputs remain possible; adversarial prompts can bypass safeguards.\n",
       "- Up-to-date knowledge: most base models are static and need retrieval or continuous fine-tuning to stay current.\n",
       "- Evaluation: benchmarks don‚Äôt fully capture real-world performance; models can ‚Äúgame‚Äù tests.\n",
       "- Cost & scale: very large models remain expensive to train; serving many users with low latency can be costly without optimization.\n",
       "\n",
       "Where R&D and product development are focused\n",
       "- Tool-enabled agents: chaining LLMs with search, calculators, databases, and APIs; agent frameworks that plan and act.\n",
       "- Better grounding: tighter integration with retrieval, knowledge graphs, and verification modules.\n",
       "- Smaller + better: research into small, efficient models that match large model performance for specific tasks.\n",
       "- Alignment and oversight: safer RLHF variants, automated monitoring, red teaming, and human-in-the-loop workflows.\n",
       "- Multimodal agents and perception: leveraging image, video, and speech to expand applications.\n",
       "- Responsible deployment: privacy-preserving inference, watermarking, provenance, and regulatory compliance.\n",
       "\n",
       "Practical advice for using LLMs today\n",
       "- Use retrieval or tools for factual tasks. Don‚Äôt rely on a raw base model for current or critical facts without verification.\n",
       "- Prefer instruction-tuned models for chat/assistant behavior.\n",
       "- Add guardrails: output filters, prompt-level constraints, and human review where mistakes matter.\n",
       "- Cost/latency: consider quantized models or smaller tuned variants for inference; use caching for repeated queries.\n",
       "- Evaluate with your data: run domain-specific tests and monitor performance and hallucinations in production.\n",
       "- Version and log prompts: track prompt/response pairs and model versions for debugging and compliance.\n",
       "\n",
       "How to pick a model/provider\n",
       "- For accuracy + convenience: commercial APIs (OpenAI, Anthropic, Google) ‚Äî strong tooling and safety features.\n",
       "- For control and on-prem: open-source families (various big models and smaller tuned ones) + fine-tuning adapters.\n",
       "- For cost-sensitive inference: go with quantized, trimmed, or distillation-based variants and use batching/accelerators.\n",
       "- For multimodal tasks: choose models or providers that explicitly support image/audio inputs and tool integration.\n",
       "\n",
       "How to evaluate LLMs (quick checklist)\n",
       "- Task accuracy on held-out, domain-specific tests\n",
       "- Hallucination and factuality checks (e.g., RAG+ground-truth comparison)\n",
       "- Latency, throughput, and cost per request\n",
       "- Safety tests: adversarial prompts, bias/toxicity scans\n",
       "- Human evaluation for helpfulness and coherence\n",
       "\n",
       "Where to follow ongoing developments\n",
       "- Company research blogs (OpenAI, Anthropic, DeepMind, Meta, Mistral)\n",
       "- ML conferences (NeurIPS, ICLR, ACL) and preprint servers (arXiv)\n",
       "- Community newsletters and dashboards (papers summaries, model release trackers)\n",
       "\n",
       "If you want, I can:\n",
       "- Summarize major open-source and commercial models up to mid‚Äë2024 and pros/cons of each.\n",
       "- Recommend a specific model or deployment approach for a task you care about (chatbot, coding assistant, search augmentation, on-device inference).\n",
       "Which would be most useful?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##LANGCHAIN##\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-5-mini')\n",
    "response = llm.invoke(\"How LLMs are doing these days\")\n",
    "\n",
    "display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6d3e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install litellm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8415c756",
   "metadata": {},
   "outputs": [],
   "source": [
    "tell_a_joke = [{\n",
    "  'role':'user','content':'Tell me a joke for a student on a journey to become LLM expert'\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83083ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import completion\n",
    "response = completion(model=\"openai/gpt-4.1-mini\")\n",
    "llm.invoke(tell_a_joke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43d8755",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e615ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
