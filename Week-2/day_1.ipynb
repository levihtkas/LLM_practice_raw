{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7e4c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97932953",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"wanj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c5c1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import requests \n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display,Markdown\n",
    "load_dotenv(override=True)\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9c2af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "anthropic_url = \"https://api.anthropic.com/v1/\"\n",
    "gemini_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "\n",
    "openai = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "gemini = OpenAI(api_key=google_api_key,base_url=gemini_url)\n",
    "\n",
    "claude = OpenAI(api_key=anthropic_api_key,base_url=anthropic_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1595a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{'role':'user','content':'Tell me a joke about LLM engineering'},{'role':'system','content':'You have to make them learn LLM'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38e6710",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.chat.completions.create(model='gpt-4.1-mini',messages=messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6f9e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b324f65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = claude.chat.completions.create(model='claude-sonnet-4-5-20250929',messages=messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89689f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb608680",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{'role':'user','content':'Tell me a joke about LLM engineering'},{'role':'system','content':'You have to make them learn LLM in JSON'}]\n",
    "\n",
    "res = openai.chat.completions.create(model=\"gpt-4.1-nano\",messages=messages,response_format={ \"type\": \"json_object\" })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b4f491",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(res.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ede2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inferences Time Vs Training Time\n",
    "# reasoning effort ssays about trade off between Inferences time and training time \n",
    "\n",
    "## Lets take puzzles to make it fun \n",
    "easy_puzzle = [\n",
    "    {\"role\": \"user\", \"content\": \n",
    "        \"You toss 2 coins. One of them is heads. What's the probability the other is tails? Answer with the probability only.\"},\n",
    "]\n",
    "\n",
    "hard = \"\"\"\n",
    "On a bookshelf, two volumes of Pushkin stand side by side: the first and the second.\n",
    "The pages of each volume together have a thickness of 2 cm, and each cover is 2 mm thick.\n",
    "A worm gnawed (perpendicular to the pages) from the first page of the first volume to the last page of the second volume.\n",
    "What distance did it gnaw through?\n",
    "\"\"\"\n",
    "\n",
    "hard_puzzle = [\n",
    "    {'role':'user','content':hard}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d5acb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now with reasoning_effort we can limit the resposnse time of an LLM therby making it answer quickly \n",
    "response = openai.chat.completions.create(model = 'gpt-5-mini',messages=hard_puzzle,reasoning_effort=\"medium\")\n",
    "\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526d92d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "requests.get(\"http://localhost:11434/\").content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fff53eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0535e520",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama = OpenAI(base_url = \"http://localhost:11434/v1\",api_key='des')\n",
    "# response = ollama.chat.completions.create(model='llama3.2',messages=easy_puzzle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fe8c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ollama.chat.completions.create(model=\"llama3.2\", messages=easy_puzzle)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d5d1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ollama.chat.completions.create(model='llama3.2',messages=easy_puzzle,response_format= {'type':'json_object'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b1d66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae8f69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install google-genai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df949620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are primarly using the openai API key now for context purpose look at some \n",
    "\n",
    "from google import genai \n",
    "\n",
    "client = genai.Client()\n",
    "#genai its models.generate_content\n",
    "response = client.models.generate_content(\n",
    "  model = 'gemini-2.5-flash-lite',\n",
    "  contents = \"Describe Tamil Nadu to someone who thinks India is unhygenic and not safe\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a035b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown((response.text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec26ae8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cadc08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import Anthropic\n",
    "\n",
    "client = Anthropic()\n",
    "\n",
    "response = client.messages.create(\n",
    "  model = 'claude-haiku-4-5',\n",
    "  messages=[{\"role\": \"user\", \"content\": \"Describe the color Blue to someone who's never been able to see in 1 sentence\"}],\n",
    "    max_tokens=100\n",
    ")\n",
    "# os.getenv('ANTHROPIC_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f40995",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d71d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(response.content[0].text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c23088",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openrouter.chat.completions.create(model=\"z-ai/glm-4.5\", messages=tell_a_joke)\n",
    "display(Markdown(response.choices[0].message.content))\n",
    "#No API key just for educational purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65fa990",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314f9c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "##LANGCHAIN##\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-5-mini')\n",
    "response = llm.invoke(\"How LLMs are doing these days\")\n",
    "\n",
    "display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6d3e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install litellm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8415c756",
   "metadata": {},
   "outputs": [],
   "source": [
    "tell_a_joke = [\n",
    "    {\"role\": \"user\", \"content\": \"Tell a joke for a student on the journey to becoming an expert in LLM Engineering\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83083ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import completion\n",
    "response = completion(model=\"openai/gpt-4.1\",messages=tell_a_joke)\n",
    "reply = response.choices[0].message.content\n",
    "display(Markdown(reply))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e615ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Total tokens:{response.usage.total_tokens}\")\n",
    "print(f\"Total Cost: {response._hidden_params['response_cost']*100:.4f} cents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcdfb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now to understand caching \n",
    "## remember to keep to your static content at the top and dynamic input at the end like a comprehendion passage context -> question -> (Your inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fff4899",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = [{\"role\": \"user\", \"content\": \"In Hamlet, when Laertes asks 'Where is my father?' what is the reply?\"}]\n",
    "\n",
    "response = completion(model=\"gemini/gemini-2.5-flash-lite\",messages=question)\n",
    "\n",
    "display(Markdown(response.choices[0].message.content))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f75412",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Total tokens: {response.usage.total_tokens}\")\n",
    "print(f'Total cost: {response._hidden_params[\"response_cost\"] * 90}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dd784d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72fada03",
   "metadata": {},
   "source": [
    "\n",
    "> Cache hits are only possible for exact prefix matches within a prompt. To realize caching benefits, place static content like instructions and examples at the beginning of your prompt, and put variable content, such as user-specific information, at the end. This also applies to images and tools, which must be identical between requests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb3e1bf",
   "metadata": {},
   "source": [
    "## Two Way Convo Chat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0751842",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_model = \"gpt-4.1-mini\"\n",
    "claude_model = \"claude-3-5-haiku-latest\"\n",
    "\n",
    "\n",
    "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way\"\n",
    "\n",
    "claude_system = 'You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.'\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced067a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "  messages = [{'role':'system','content':gpt_system}]\n",
    "\n",
    "  for gpt,claude in zip(gpt_messages,claude_messages):\n",
    "    messages.append({'role':'assistant','content':gpt})\n",
    "    messages.append({'role':'user','content':claude})\n",
    "  response = openai.chat.completions.create(model=gpt_model,messages=messages)\n",
    "  return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce2a1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "anthropic = OpenAI(base_url=anthropic_url,api_key=anthropic_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffc18a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "  messages = [{'role':'system','content':claude_system}]\n",
    "  for gpt,claude_message in zip(gpt_messages,claude_messages):\n",
    "    messages.append({'role':'user','content':gpt})\n",
    "    messages.append({'role':'assistant','content':claude_message})\n",
    "  messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "  print(\"Claude's mindvoice:\", *messages[-2:], sep=\"\\n\")\n",
    "  response = anthropic.chat.completions.create(model=claude_model,messages=messages)\n",
    "  return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e0bf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = [{\"role\": \"system\", \"content\": claude_system}]\n",
    "    for gpt, claude_message in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    response = anthropic.chat.completions.create(model=claude_model, messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3980ddab",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = call_gpt()\n",
    "gpt_messages.append(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2be0648",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae46345b",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = call_claude()\n",
    "claude_messages.append(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8f1ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfaa5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    gpt_next = call_gpt()\n",
    "    display(Markdown(f\"### GPT:\\n{gpt_next}\\n\"))\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    display(Markdown(f\"### Claude:\\n{claude_next}\\n\"))\n",
    "    claude_messages.append(claude_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d5dbe3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5511853c",
   "metadata": {},
   "source": [
    "### THREE WAY CHAT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b0615d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d7f659",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_model = \"gpt-4.1-mini\"\n",
    "claude_model = \"claude-3-5-haiku-latest\"\n",
    "gemini_model = \"gemini-2.5-flash\"\n",
    "\n",
    "gemini_call = OpenAI(base_url=gemini_url,api_key=google_api_key)\n",
    "\n",
    "\n",
    "gpt_system = (\n",
    "    \"You are Annamalai. You are playing a word-guessing game with Bhanu and Chandru. \"\n",
    "    \"You think of a secret word. Do NOT reveal the word in your response. \"\n",
    "    \"Give clues only. Bhanu and Chandru must try to guess the word based on your clues. \"\n",
    "    \"Each of them gets exactly 2 chances. After both have used 2 guesses each, \"\n",
    "    \"you must reveal the correct answer. \"\n",
    "    \"Also tell them after each guess whether they are right or wrong.\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "claude_system = (\n",
    "    \"You are Bhanu. You are a player in a word-guessing game. Chandru is your team\"\n",
    "    \"Annamalai will give clues. You must guess the word based on the clues. \"\n",
    "    \"You get exactly 2 chances. Keep your guesses short and clear. \"\n",
    "    \"Do not reveal or invent the answer yourself. Just guess based on clues.\"\n",
    ")\n",
    "\n",
    "gemini_system = (\n",
    "    \"You are Chandru. You are a player in a word-guessing game. Bhanu is your team\"\n",
    "    \"Annamalai will give clues. You must guess the word based on the clues. \"\n",
    "    \"You get exactly 2 chances. Keep your guesses short and confident. \"\n",
    "    \"guess based on the clues. You must only guess\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "gpt_messages = [\"Hi\"]\n",
    "claude_messages = [\"Hello gang\"]\n",
    "gemini_messages = [\"Hey there\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321c2376",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "  messages = [{'role':'system','content':gpt_system}]\n",
    "  for gpt,claude,gemini in zip(gpt_messages,claude_messages,gemini_messages):\n",
    "    messages.append({\"role\":'assistant','content':gpt})\n",
    "    messages.append({\"role\":'user','content':claude})\n",
    "    messages.append({\"role\":'user','content':gemini})\n",
    "  response = openai.chat.completions.create(model=gpt_model,messages=messages)\n",
    "\n",
    "  return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def call_claude():\n",
    "  messages = [{'role':'system','content':claude_system}]\n",
    "  for gpt,claude,gemini in zip(gpt_messages,claude_messages,gemini_messages):\n",
    "    messages.append({'role':'assistant','content':\"Annamali \"+gpt})\n",
    "    messages.append({\"role\":'user','content':\"Bhanu \"+claude})\n",
    "    messages.append({\"role\":'user','content':\"Chandru \"+gemini})\n",
    "  messages.append({'role':'assistant','content':\"Annamali \"+gpt_messages[-1]})\n",
    "  response = anthropic.chat.completions.create(model=claude_model,messages=messages)\n",
    "  return response.choices[0].message.content\n",
    "\n",
    "def call_gemini():\n",
    "  messages = [{'role':'system','content':gemini_system}]\n",
    "  for gpt,claude,gemini in zip(gpt_messages,claude_messages,gemini_messages):\n",
    "    messages.append({'role':'assistant','content':\"Annamali \"+gpt})\n",
    "    messages.append({\"role\":'user','content':\"Bhanu \"+claude})\n",
    "    messages.append({\"role\":'user','content':\"Chandru \"+gemini})\n",
    "  messages.append({'role':'assistant','content':\"Annamali \"+gpt_messages[-1]})\n",
    "  # print(gpt[-1])\n",
    "  messages.append({\"role\":'user','content':\"Bhanu \"+claude_messages[-1]})\n",
    "  # print(claude[-1])\n",
    "  response = gemini_call.chat.completions.create(model=gemini_model,messages=messages)\n",
    "  return response.choices[0].message.content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf69786",
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_chat = call_gpt()\n",
    "display(Markdown(f\"### ðŸ¤– ChatGPT Response:\\n\\n{rep_chat}\"))\n",
    "gpt_messages.append(rep_chat)\n",
    "\n",
    "rep_claude = call_claude()\n",
    "display(Markdown(f\"### ðŸ§  Claude Response:\\n\\n{rep_claude}\"))\n",
    "claude_messages.append(rep_claude)\n",
    "\n",
    "rep_gemini = call_gemini()\n",
    "display(Markdown(f\"### ðŸŒŸ Gemini Response:\\n\\n{rep_gemini}\"))\n",
    "gemini_messages.append(rep_gemini)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22148cbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caea6995",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4848e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf6ea0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16957a8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
