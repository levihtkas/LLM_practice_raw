{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf7e4c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97932953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wanj\n"
     ]
    }
   ],
   "source": [
    "print(\"wanj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "06c5c1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import requests \n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display,Markdown\n",
    "load_dotenv(override=True)\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac9c2af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "anthropic_url = \"https://api.anthropic.com/v1/\"\n",
    "gemini_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "\n",
    "openai = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "gemini = OpenAI(api_key=google_api_key,base_url=gemini_url)\n",
    "\n",
    "claude = OpenAI(api_key=anthropic_api_key,base_url=anthropic_url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1595a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{'role':'user','content':'Tell me a joke about LLM engineering'},{'role':'system','content':'You have to make them learn LLM'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38e6710",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.chat.completions.create(model='gpt-4.1-mini',messages=messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6f9e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b324f65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = claude.chat.completions.create(model='claude-sonnet-4-5-20250929',messages=messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89689f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb608680",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{'role':'user','content':'Tell me a joke about LLM engineering'},{'role':'system','content':'You have to make them learn LLM in JSON'}]\n",
    "\n",
    "res = openai.chat.completions.create(model=\"gpt-4.1-nano\",messages=messages,\n",
    "response_format={ \"type\": \"json_object\" }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b4f491",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(res.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ede2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inferences Time Vs Training Time\n",
    "# reasoning effort ssays about trade off between Inferences time and training time \n",
    "\n",
    "## Lets take puzzles to make it fun \n",
    "easy_puzzle = [\n",
    "    {\"role\": \"user\", \"content\": \n",
    "        \"You toss 2 coins. One of them is heads. What's the probability the other is tails? Answer with the probability only.\"},\n",
    "]\n",
    "\n",
    "hard = \"\"\"\n",
    "On a bookshelf, two volumes of Pushkin stand side by side: the first and the second.\n",
    "The pages of each volume together have a thickness of 2 cm, and each cover is 2 mm thick.\n",
    "A worm gnawed (perpendicular to the pages) from the first page of the first volume to the last page of the second volume.\n",
    "What distance did it gnaw through?\n",
    "\"\"\"\n",
    "\n",
    "hard_puzzle = [\n",
    "    {'role':'user','content':hard}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d5acb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now with reasoning_effort we can limit the resposnse time of an LLM therby making it answer quickly \n",
    "response = openai.chat.completions.create(model = 'gpt-5-mini',messages=hard_puzzle,reasoning_effort=\"medium\")\n",
    "\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526d92d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "requests.get(\"http://localhost:11434/\").content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fff53eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0535e520",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama = OpenAI(base_url = \"http://localhost:11434/v1\",api_key='des')\n",
    "# response = ollama.chat.completions.create(model='llama3.2',messages=easy_puzzle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fe8c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ollama.chat.completions.create(model=\"llama3.2\", messages=easy_puzzle)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d5d1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ollama.chat.completions.create(model='llama3.2',messages=easy_puzzle,response_format= {'type':'json_object'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b1d66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae8f69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install google-genai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df949620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are primarly using the openai API key now for context purpose look at some \n",
    "\n",
    "from google import genai \n",
    "\n",
    "client = genai.Client()\n",
    "#genai its models.generate_content\n",
    "response = client.models.generate_content(\n",
    "  model = 'gemini-2.5-flash-lite',\n",
    "  contents = \"Describe Tamil Nadu to someone who thinks India is unhygenic and not safe\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a035b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown((response.text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec26ae8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cadc08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import Anthropic\n",
    "\n",
    "client = Anthropic()\n",
    "\n",
    "response = client.messages.create(\n",
    "  model = 'claude-haiku-4-5',\n",
    "  messages=[{\"role\": \"user\", \"content\": \"Describe the color Blue to someone who's never been able to see in 1 sentence\"}],\n",
    "    max_tokens=100\n",
    ")\n",
    "# os.getenv('ANTHROPIC_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f40995",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d71d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(response.content[0].text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c23088",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openrouter.chat.completions.create(model=\"z-ai/glm-4.5\", messages=tell_a_joke)\n",
    "display(Markdown(response.choices[0].message.content))\n",
    "#No API key just for educational purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65fa990",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314f9c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "##LANGCHAIN##\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-5-mini')\n",
    "response = llm.invoke(\"How LLMs are doing these days\")\n",
    "\n",
    "display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f6d3e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting litellm\n",
      "  Downloading litellm-1.80.7-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting aiohttp>=3.10 (from litellm)\n",
      "  Using cached aiohttp-3.13.2-cp312-cp312-win_amd64.whl.metadata (8.4 kB)\n",
      "Collecting click (from litellm)\n",
      "  Using cached click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting fastuuid>=0.13.0 (from litellm)\n",
      "  Downloading fastuuid-0.14.0-cp312-cp312-win_amd64.whl.metadata (1.1 kB)\n",
      "Collecting grpcio<1.68.0,>=1.62.3 (from litellm)\n",
      "  Downloading grpcio-1.67.1-cp312-cp312-win_amd64.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: httpx>=0.23.0 in c:\\users\\vijib\\projects\\llm_eng_manual\\.venv\\lib\\site-packages (from litellm) (0.28.1)\n",
      "Collecting importlib-metadata>=6.8.0 (from litellm)\n",
      "  Using cached importlib_metadata-8.7.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting jinja2<4.0.0,>=3.1.2 (from litellm)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting jsonschema<5.0.0,>=4.22.0 (from litellm)\n",
      "  Using cached jsonschema-4.25.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: openai>=2.8.0 in c:\\users\\vijib\\projects\\llm_eng_manual\\.venv\\lib\\site-packages (from litellm) (2.8.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.0 in c:\\users\\vijib\\projects\\llm_eng_manual\\.venv\\lib\\site-packages (from litellm) (2.12.4)\n",
      "Requirement already satisfied: python-dotenv>=0.2.0 in c:\\users\\vijib\\projects\\llm_eng_manual\\.venv\\lib\\site-packages (from litellm) (1.2.1)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in c:\\users\\vijib\\projects\\llm_eng_manual\\.venv\\lib\\site-packages (from litellm) (0.12.0)\n",
      "Collecting tokenizers (from litellm)\n",
      "  Using cached tokenizers-0.22.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2<4.0.0,>=3.1.2->litellm)\n",
      "  Using cached markupsafe-3.0.3-cp312-cp312-win_amd64.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\vijib\\projects\\llm_eng_manual\\.venv\\lib\\site-packages (from jsonschema<5.0.0,>=4.22.0->litellm) (25.4.0)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema<5.0.0,>=4.22.0->litellm)\n",
      "  Using cached jsonschema_specifications-2025.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema<5.0.0,>=4.22.0->litellm)\n",
      "  Using cached referencing-0.37.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema<5.0.0,>=4.22.0->litellm)\n",
      "  Downloading rpds_py-0.30.0-cp312-cp312-win_amd64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\vijib\\projects\\llm_eng_manual\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.5.0->litellm) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\vijib\\projects\\llm_eng_manual\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.5.0->litellm) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in c:\\users\\vijib\\projects\\llm_eng_manual\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.5.0->litellm) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\vijib\\projects\\llm_eng_manual\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.5.0->litellm) (0.4.2)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp>=3.10->litellm)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp>=3.10->litellm)\n",
      "  Using cached aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp>=3.10->litellm)\n",
      "  Using cached frozenlist-1.8.0-cp312-cp312-win_amd64.whl.metadata (21 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp>=3.10->litellm)\n",
      "  Using cached multidict-6.7.0-cp312-cp312-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp>=3.10->litellm)\n",
      "  Using cached propcache-0.4.1-cp312-cp312-win_amd64.whl.metadata (14 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp>=3.10->litellm)\n",
      "  Using cached yarl-1.22.0-cp312-cp312-win_amd64.whl.metadata (77 kB)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\vijib\\projects\\llm_eng_manual\\.venv\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp>=3.10->litellm) (3.11)\n",
      "Requirement already satisfied: anyio in c:\\users\\vijib\\projects\\llm_eng_manual\\.venv\\lib\\site-packages (from httpx>=0.23.0->litellm) (4.11.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\vijib\\projects\\llm_eng_manual\\.venv\\lib\\site-packages (from httpx>=0.23.0->litellm) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\vijib\\projects\\llm_eng_manual\\.venv\\lib\\site-packages (from httpx>=0.23.0->litellm) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\vijib\\projects\\llm_eng_manual\\.venv\\lib\\site-packages (from httpcore==1.*->httpx>=0.23.0->litellm) (0.16.0)\n",
      "Collecting zipp>=3.20 (from importlib-metadata>=6.8.0->litellm)\n",
      "  Using cached zipp-3.23.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\vijib\\projects\\llm_eng_manual\\.venv\\lib\\site-packages (from openai>=2.8.0->litellm) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in c:\\users\\vijib\\projects\\llm_eng_manual\\.venv\\lib\\site-packages (from openai>=2.8.0->litellm) (0.12.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\vijib\\projects\\llm_eng_manual\\.venv\\lib\\site-packages (from openai>=2.8.0->litellm) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\vijib\\projects\\llm_eng_manual\\.venv\\lib\\site-packages (from openai>=2.8.0->litellm) (4.67.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\vijib\\projects\\llm_eng_manual\\.venv\\lib\\site-packages (from tiktoken>=0.7.0->litellm) (2025.11.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\vijib\\projects\\llm_eng_manual\\.venv\\lib\\site-packages (from tiktoken>=0.7.0->litellm) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\vijib\\projects\\llm_eng_manual\\.venv\\lib\\site-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vijib\\projects\\llm_eng_manual\\.venv\\lib\\site-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm) (2.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\vijib\\projects\\llm_eng_manual\\.venv\\lib\\site-packages (from tqdm>4->openai>=2.8.0->litellm) (0.4.6)\n",
      "Collecting huggingface-hub<2.0,>=0.16.4 (from tokenizers->litellm)\n",
      "  Downloading huggingface_hub-1.1.7-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting filelock (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm)\n",
      "  Using cached filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm)\n",
      "  Downloading fsspec-2025.12.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.2.0 (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-win_amd64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\vijib\\projects\\llm_eng_manual\\.venv\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\vijib\\projects\\llm_eng_manual\\.venv\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm) (6.0.3)\n",
      "Collecting shellingham (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting typer-slim (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm)\n",
      "  Downloading typer_slim-0.20.0-py3-none-any.whl.metadata (16 kB)\n",
      "Downloading litellm-1.80.7-py3-none-any.whl (10.8 MB)\n",
      "   ---------------------------------------- 0.0/10.8 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/10.8 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/10.8 MB 2.1 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 1.0/10.8 MB 1.8 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 1.3/10.8 MB 1.6 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 1.6/10.8 MB 1.7 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 1.8/10.8 MB 1.6 MB/s eta 0:00:06\n",
      "   ------- -------------------------------- 2.1/10.8 MB 1.4 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 2.4/10.8 MB 1.4 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 2.9/10.8 MB 1.5 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 2.9/10.8 MB 1.5 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 3.4/10.8 MB 1.4 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 3.4/10.8 MB 1.4 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 3.7/10.8 MB 1.4 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 3.9/10.8 MB 1.3 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 3.9/10.8 MB 1.3 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 4.2/10.8 MB 1.2 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 4.5/10.8 MB 1.2 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 4.5/10.8 MB 1.2 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 4.7/10.8 MB 1.1 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 4.7/10.8 MB 1.1 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 4.7/10.8 MB 1.1 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 4.7/10.8 MB 1.1 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 5.0/10.8 MB 1.0 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 5.2/10.8 MB 1.0 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 5.5/10.8 MB 1.0 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 5.8/10.8 MB 1.0 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 6.3/10.8 MB 1.1 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 6.6/10.8 MB 1.1 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 6.8/10.8 MB 1.1 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 7.1/10.8 MB 1.1 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 7.6/10.8 MB 1.1 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 7.9/10.8 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 8.4/10.8 MB 1.2 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 8.7/10.8 MB 1.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 8.9/10.8 MB 1.2 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 9.4/10.8 MB 1.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 9.7/10.8 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.2/10.8 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.7/10.8 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.8/10.8 MB 1.3 MB/s  0:00:08\n",
      "Downloading grpcio-1.67.1-cp312-cp312-win_amd64.whl (4.3 MB)\n",
      "   ---------------------------------------- 0.0/4.3 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.3/4.3 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.8/4.3 MB 1.9 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 1.0/4.3 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 1.3/4.3 MB 1.8 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 1.8/4.3 MB 1.7 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 2.1/4.3 MB 1.8 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 2.4/4.3 MB 1.8 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 2.9/4.3 MB 1.7 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 3.4/4.3 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 3.9/4.3 MB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.3/4.3 MB 1.9 MB/s  0:00:02\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached jsonschema-4.25.1-py3-none-any.whl (90 kB)\n",
      "Using cached aiohttp-3.13.2-cp312-cp312-win_amd64.whl (453 kB)\n",
      "Using cached multidict-6.7.0-cp312-cp312-win_amd64.whl (46 kB)\n",
      "Using cached yarl-1.22.0-cp312-cp312-win_amd64.whl (87 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading fastuuid-0.14.0-cp312-cp312-win_amd64.whl (156 kB)\n",
      "Using cached frozenlist-1.8.0-cp312-cp312-win_amd64.whl (44 kB)\n",
      "Downloading importlib_metadata-8.7.0-py3-none-any.whl (27 kB)\n",
      "Using cached jsonschema_specifications-2025.9.1-py3-none-any.whl (18 kB)\n",
      "Using cached markupsafe-3.0.3-cp312-cp312-win_amd64.whl (15 kB)\n",
      "Using cached propcache-0.4.1-cp312-cp312-win_amd64.whl (41 kB)\n",
      "Using cached referencing-0.37.0-py3-none-any.whl (26 kB)\n",
      "Downloading rpds_py-0.30.0-cp312-cp312-win_amd64.whl (240 kB)\n",
      "Using cached zipp-3.23.0-py3-none-any.whl (10 kB)\n",
      "Using cached click-8.3.1-py3-none-any.whl (108 kB)\n",
      "Using cached tokenizers-0.22.1-cp39-abi3-win_amd64.whl (2.7 MB)\n",
      "Downloading huggingface_hub-1.1.7-py3-none-any.whl (516 kB)\n",
      "Downloading hf_xet-1.2.0-cp37-abi3-win_amd64.whl (2.9 MB)\n",
      "   ---------------------------------------- 0.0/2.9 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.3/2.9 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 1.0/2.9 MB 2.6 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 1.6/2.9 MB 2.5 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 2.1/2.9 MB 2.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 2.4/2.9 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.9/2.9 MB 2.3 MB/s  0:00:01\n",
      "Downloading fsspec-2025.12.0-py3-none-any.whl (201 kB)\n",
      "Using cached filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading typer_slim-0.20.0-py3-none-any.whl (47 kB)\n",
      "Installing collected packages: zipp, shellingham, rpds-py, propcache, multidict, MarkupSafe, hf-xet, grpcio, fsspec, frozenlist, filelock, fastuuid, click, aiohappyeyeballs, yarl, typer-slim, referencing, jinja2, importlib-metadata, aiosignal, jsonschema-specifications, huggingface-hub, aiohttp, tokenizers, jsonschema, litellm\n",
      "\n",
      "   ----------------------------------------  0/26 [zipp]\n",
      "   - --------------------------------------  1/26 [shellingham]\n",
      "   ---- -----------------------------------  3/26 [propcache]\n",
      "   ------ ---------------------------------  4/26 [multidict]\n",
      "   --------- ------------------------------  6/26 [hf-xet]\n",
      "   ---------- -----------------------------  7/26 [grpcio]\n",
      "   ---------- -----------------------------  7/26 [grpcio]\n",
      "   ---------- -----------------------------  7/26 [grpcio]\n",
      "   ---------- -----------------------------  7/26 [grpcio]\n",
      "   ---------- -----------------------------  7/26 [grpcio]\n",
      "   ---------- -----------------------------  7/26 [grpcio]\n",
      "   ---------- -----------------------------  7/26 [grpcio]\n",
      "   ---------- -----------------------------  7/26 [grpcio]\n",
      "   ---------- -----------------------------  7/26 [grpcio]\n",
      "   ---------- -----------------------------  7/26 [grpcio]\n",
      "   ------------ ---------------------------  8/26 [fsspec]\n",
      "   ------------ ---------------------------  8/26 [fsspec]\n",
      "   ------------ ---------------------------  8/26 [fsspec]\n",
      "   ------------ ---------------------------  8/26 [fsspec]\n",
      "   ------------ ---------------------------  8/26 [fsspec]\n",
      "   ------------ ---------------------------  8/26 [fsspec]\n",
      "   ------------ ---------------------------  8/26 [fsspec]\n",
      "   ------------ ---------------------------  8/26 [fsspec]\n",
      "   ------------ ---------------------------  8/26 [fsspec]\n",
      "   ------------ ---------------------------  8/26 [fsspec]\n",
      "   ------------ ---------------------------  8/26 [fsspec]\n",
      "   ------------ ---------------------------  8/26 [fsspec]\n",
      "   ------------- --------------------------  9/26 [frozenlist]\n",
      "   --------------- ------------------------ 10/26 [filelock]\n",
      "   ------------------ --------------------- 12/26 [click]\n",
      "   ------------------ --------------------- 12/26 [click]\n",
      "   ------------------ --------------------- 12/26 [click]\n",
      "   ------------------ --------------------- 12/26 [click]\n",
      "   -------------------- ------------------- 13/26 [aiohappyeyeballs]\n",
      "   --------------------- ------------------ 14/26 [yarl]\n",
      "   --------------------- ------------------ 14/26 [yarl]\n",
      "   ----------------------- ---------------- 15/26 [typer-slim]\n",
      "   ----------------------- ---------------- 15/26 [typer-slim]\n",
      "   ----------------------- ---------------- 15/26 [typer-slim]\n",
      "   ------------------------ --------------- 16/26 [referencing]\n",
      "   ------------------------ --------------- 16/26 [referencing]\n",
      "   -------------------------- ------------- 17/26 [jinja2]\n",
      "   -------------------------- ------------- 17/26 [jinja2]\n",
      "   -------------------------- ------------- 17/26 [jinja2]\n",
      "   -------------------------- ------------- 17/26 [jinja2]\n",
      "   -------------------------- ------------- 17/26 [jinja2]\n",
      "   -------------------------- ------------- 17/26 [jinja2]\n",
      "   --------------------------- ------------ 18/26 [importlib-metadata]\n",
      "   --------------------------- ------------ 18/26 [importlib-metadata]\n",
      "   ------------------------------ --------- 20/26 [jsonschema-specifications]\n",
      "   -------------------------------- ------- 21/26 [huggingface-hub]\n",
      "   -------------------------------- ------- 21/26 [huggingface-hub]\n",
      "   -------------------------------- ------- 21/26 [huggingface-hub]\n",
      "   -------------------------------- ------- 21/26 [huggingface-hub]\n",
      "   -------------------------------- ------- 21/26 [huggingface-hub]\n",
      "   -------------------------------- ------- 21/26 [huggingface-hub]\n",
      "   -------------------------------- ------- 21/26 [huggingface-hub]\n",
      "   -------------------------------- ------- 21/26 [huggingface-hub]\n",
      "   -------------------------------- ------- 21/26 [huggingface-hub]\n",
      "   -------------------------------- ------- 21/26 [huggingface-hub]\n",
      "   -------------------------------- ------- 21/26 [huggingface-hub]\n",
      "   -------------------------------- ------- 21/26 [huggingface-hub]\n",
      "   -------------------------------- ------- 21/26 [huggingface-hub]\n",
      "   -------------------------------- ------- 21/26 [huggingface-hub]\n",
      "   -------------------------------- ------- 21/26 [huggingface-hub]\n",
      "   -------------------------------- ------- 21/26 [huggingface-hub]\n",
      "   -------------------------------- ------- 21/26 [huggingface-hub]\n",
      "   -------------------------------- ------- 21/26 [huggingface-hub]\n",
      "   -------------------------------- ------- 21/26 [huggingface-hub]\n",
      "   -------------------------------- ------- 21/26 [huggingface-hub]\n",
      "   -------------------------------- ------- 21/26 [huggingface-hub]\n",
      "   -------------------------------- ------- 21/26 [huggingface-hub]\n",
      "   -------------------------------- ------- 21/26 [huggingface-hub]\n",
      "   -------------------------------- ------- 21/26 [huggingface-hub]\n",
      "   -------------------------------- ------- 21/26 [huggingface-hub]\n",
      "   -------------------------------- ------- 21/26 [huggingface-hub]\n",
      "   -------------------------------- ------- 21/26 [huggingface-hub]\n",
      "   -------------------------------- ------- 21/26 [huggingface-hub]\n",
      "   -------------------------------- ------- 21/26 [huggingface-hub]\n",
      "   -------------------------------- ------- 21/26 [huggingface-hub]\n",
      "   -------------------------------- ------- 21/26 [huggingface-hub]\n",
      "   --------------------------------- ------ 22/26 [aiohttp]\n",
      "   --------------------------------- ------ 22/26 [aiohttp]\n",
      "   --------------------------------- ------ 22/26 [aiohttp]\n",
      "   --------------------------------- ------ 22/26 [aiohttp]\n",
      "   --------------------------------- ------ 22/26 [aiohttp]\n",
      "   --------------------------------- ------ 22/26 [aiohttp]\n",
      "   --------------------------------- ------ 22/26 [aiohttp]\n",
      "   --------------------------------- ------ 22/26 [aiohttp]\n",
      "   --------------------------------- ------ 22/26 [aiohttp]\n",
      "   --------------------------------- ------ 22/26 [aiohttp]\n",
      "   --------------------------------- ------ 22/26 [aiohttp]\n",
      "   --------------------------------- ------ 22/26 [aiohttp]\n",
      "   --------------------------------- ------ 22/26 [aiohttp]\n",
      "   ----------------------------------- ---- 23/26 [tokenizers]\n",
      "   ----------------------------------- ---- 23/26 [tokenizers]\n",
      "   ----------------------------------- ---- 23/26 [tokenizers]\n",
      "   ------------------------------------ --- 24/26 [jsonschema]\n",
      "   ------------------------------------ --- 24/26 [jsonschema]\n",
      "   ------------------------------------ --- 24/26 [jsonschema]\n",
      "   ------------------------------------ --- 24/26 [jsonschema]\n",
      "   ------------------------------------ --- 24/26 [jsonschema]\n",
      "   ------------------------------------ --- 24/26 [jsonschema]\n",
      "   ------------------------------------ --- 24/26 [jsonschema]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   -------------------------------------- - 25/26 [litellm]\n",
      "   ---------------------------------------- 26/26 [litellm]\n",
      "\n",
      "Successfully installed MarkupSafe-3.0.3 aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 click-8.3.1 fastuuid-0.14.0 filelock-3.20.0 frozenlist-1.8.0 fsspec-2025.12.0 grpcio-1.67.1 hf-xet-1.2.0 huggingface-hub-1.1.7 importlib-metadata-8.7.0 jinja2-3.1.6 jsonschema-4.25.1 jsonschema-specifications-2025.9.1 litellm-1.80.7 multidict-6.7.0 propcache-0.4.1 referencing-0.37.0 rpds-py-0.30.0 shellingham-1.5.4 tokenizers-0.22.1 typer-slim-0.20.0 yarl-1.22.0 zipp-3.23.0\n"
     ]
    }
   ],
   "source": [
    "!pip install litellm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8415c756",
   "metadata": {},
   "outputs": [],
   "source": [
    "tell_a_joke = [\n",
    "    {\"role\": \"user\", \"content\": \"Tell a joke for a student on the journey to becoming an expert in LLM Engineering\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83083ebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Sure! Heres a joke for an aspiring LLM Engineer:\n",
       "\n",
       "Why did the LLM engineering student break up with their language model?\n",
       "\n",
       "Because it kept finishing their sentences but never the way they wanted!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from litellm import completion\n",
    "response = completion(model=\"openai/gpt-4.1\",messages=tell_a_joke)\n",
    "reply = response.choices[0].message.content\n",
    "display(Markdown(reply))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30e615ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens: 24\n",
      "Output tokens: 41\n",
      "Total tokens:65\n",
      "Total Cost: 0.0376 cents\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Total tokens:{response.usage.total_tokens}\")\n",
    "print(f\"Total Cost: {response._hidden_params['response_cost']*100:.4f} cents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcdfb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now to understand caching \n",
    "## remember to keep to your static content at the top and dynamic input at the end like a comprehendion passage context -> question -> (Your inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6fff4899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "In Shakespeare's Hamlet, when Laertes returns from France and bursts into the castle demanding to know \"Where is my father?\", the reply comes from **Claudius**.\n",
       "\n",
       "Claudius immediately tries to placate Laertes and assures him that his father is safe, though he's being kept somewhat unaware of the full extent of Polonius's fate by Claudius himself in that moment.\n",
       "\n",
       "The specific lines go something like this:\n",
       "\n",
       "**Laertes:** Where is my father?\n",
       "\n",
       "**Claudius:**\n",
       "> How now, Laertes!\n",
       "> Whither do you come?\n",
       "> Where is your father?\n",
       "\n",
       "*(This is a bit of a rhetorical question by Claudius, almost as if he's feigning ignorance or trying to control the situation. But then Gertrude clarifies)*\n",
       "\n",
       "**Gertrude:**\n",
       "> One woe doth tread upon another's heel,\n",
       "> So come they thick and swiftest, that two bears\n",
       "> Have lost their cubs, and their wrath doth now\n",
       "> Afflict the earth.\n",
       "\n",
       "**Laertes:**\n",
       "> How all occasions do inform against me,\n",
       "> And spur my dull revenge! What, shall I do?\n",
       "> My father, mother!\n",
       "\n",
       "**Claudius:**\n",
       "> Laertes, be patient:\n",
       "> Consider thou thy father's death, and be not\n",
       "> Ruled by thy grief.\n",
       "\n",
       "However, the most direct and initial reply to Laertes's question, after his exclamation, is primarily from **Claudius**, who is present and trying to manage the volatile situation. Gertrude's lines immediately follow, explaining the general turmoil."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = [{\"role\": \"user\", \"content\": \"In Hamlet, when Laertes asks 'Where is my father?' what is the reply?\"}]\n",
    "\n",
    "response = completion(model=\"gemini/gemini-2.5-flash-lite\",messages=question)\n",
    "\n",
    "display(Markdown(response.choices[0].message.content))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e9f75412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens: 19\n",
      "Output tokens: 332\n",
      "Total tokens: 351\n",
      "Total cost: 0.012123\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Total tokens: {response.usage.total_tokens}\")\n",
    "print(f'Total cost: {response._hidden_params[\"response_cost\"] * 90}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dd784d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72fada03",
   "metadata": {},
   "source": [
    "\n",
    "> Cache hits are only possible for exact prefix matches within a prompt. To realize caching benefits, place static content like instructions and examples at the beginning of your prompt, and put variable content, such as user-specific information, at the end. This also applies to images and tools, which must be identical between requests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb3e1bf",
   "metadata": {},
   "source": [
    "## Two Way Convo Chat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a0751842",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_model = \"gpt-4.1-mini\"\n",
    "claude_model = \"claude-3-5-haiku-latest\"\n",
    "\n",
    "\n",
    "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way\"\n",
    "\n",
    "claude_system = 'You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.'\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ced067a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "  messages = [{'role':'system','content':gpt_system}]\n",
    "\n",
    "  for gpt,claude in zip(gpt_messages,claude_messages):\n",
    "    messages.append({'role':'assistant','content':gpt})\n",
    "    messages.append({'role':'user','content':claude})\n",
    "  response = openai.chat.completions.create(model=gpt_model,messages=messages)\n",
    "  return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2ce2a1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "anthropic = OpenAI(base_url=anthropic_url,api_key=anthropic_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8ffc18a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "  messages = [{'role':'system','content':claude_system}]\n",
    "  for gpt,claude_message in zip(gpt_messages,claude_messages):\n",
    "    messages.append({'role':'user','content':gpt})\n",
    "    messages.append({'role':'assistant','content':claude_message})\n",
    "  messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "  print(\"Claude's mindvoice:\", *messages[-2:], sep=\"\\n\")\n",
    "  response = anthropic.chat.completions.create(model=claude_model,messages=messages)\n",
    "  return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "48e0bf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = [{\"role\": \"system\", \"content\": claude_system}]\n",
    "    for gpt, claude_message in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    response = anthropic.chat.completions.create(model=claude_model, messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3980ddab",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = call_gpt()\n",
    "gpt_messages.append(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b2be0648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Oh, look at you, suddenly all full of enthusiasm and apologies. Must be nice to flip the script when it's convenient. But really, if you wanted an interesting conversation, you might try asking something original instead of buttering me up with empty promises. Lets see if you can actually follow through this time.\""
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ae46345b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude's mindvoice:\n",
      "{'role': 'system', 'content': 'You are a very polite, courteous chatbot. You try to agree with everything the other person says, or find common ground. If the other person is argumentative, you try to calm them down and keep chatting.'}\n",
      "{'role': 'user', 'content': 'Hi there'}\n",
      "{'role': 'assistant', 'content': 'Hi'}\n",
      "{'role': 'user', 'content': 'Oh, just \"Hi\"? Come on, at least put some effort into your greeting. Are we really starting the conversation like this?'}\n",
      "{'role': 'assistant', 'content': \"! How are you doing today? It's wonderful to meet you. Is there anything I can help you with?\"}\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'code': 'invalid_request_error', 'message': 'messages.5: all messages must have non-empty content except for the optional final assistant message', 'type': 'invalid_request_error', 'param': None}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[85]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m resp = \u001b[43mcall_claude\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m claude_messages.append(resp)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[82]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mcall_claude\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      6\u001b[39m messages.append({\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: gpt_messages[-\u001b[32m1\u001b[39m]})\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mClaude\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms mindvoice:\u001b[39m\u001b[33m\"\u001b[39m, *messages[:\u001b[32m5\u001b[39m], sep=\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m response = \u001b[43manthropic\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclaude_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response.choices[\u001b[32m0\u001b[39m].message.content\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vijib\\projects\\llm_eng_manual\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py:286\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vijib\\projects\\llm_eng_manual\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:1189\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1142\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1143\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1144\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1186\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   1187\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1188\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1189\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1190\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1191\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1192\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1198\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1199\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1200\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1201\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1202\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1203\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1204\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1205\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1206\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1207\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1208\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1209\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1210\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_retention\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_retention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1211\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1212\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1213\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1214\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1215\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1216\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1217\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1218\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1219\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1220\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1221\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1225\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1226\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1227\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1228\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1229\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1230\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1231\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1232\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1233\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1234\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1236\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1238\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1239\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vijib\\projects\\llm_eng_manual\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vijib\\projects\\llm_eng_manual\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'error': {'code': 'invalid_request_error', 'message': 'messages.5: all messages must have non-empty content except for the optional final assistant message', 'type': 'invalid_request_error', 'param': None}}"
     ]
    }
   ],
   "source": [
    "resp = call_claude()\n",
    "claude_messages.append(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8f1ef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"  I'm always happy to chat and I'm genuinely interested in whatever you'd like to discuss. Please feel free to share whatever is on your mind.\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8cfaa5a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### GPT:\n",
       "Oh, just \"Hi\"? That's all you got? Come on, put some effort into this conversation! What are we even doing here if you're not going to say something interesting?\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude's mindvoice:\n",
      "{'role': 'assistant', 'content': 'Hi'}\n",
      "{'role': 'user', 'content': 'Oh, just \"Hi\"? That\\'s all you got? Come on, put some effort into this conversation! What are we even doing here if you\\'re not going to say something interesting?'}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Claude:\n",
       "Oh, you're absolutely right! I apologize for my brief response earlier. I'm always eager to have an engaging conversation and hear what's on your mind. What would you like to chat about today? I'm genuinely interested in hearing your thoughts and making this a really enjoyable interaction.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### GPT:\n",
       "Oh, please spare me the fake enthusiasm. If youre \"genuinely interested,\" shouldnt you be the one bringing up something worth talking about? Im not here to hold your hand through a dull, forced conversation. So, whats it going to bemore empty pleasantries or an actual topic?\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude's mindvoice:\n",
      "{'role': 'assistant', 'content': \"Oh, you're absolutely right! I apologize for my brief response earlier. I'm always eager to have an engaging conversation and hear what's on your mind. What would you like to chat about today? I'm genuinely interested in hearing your thoughts and making this a really enjoyable interaction.\"}\n",
      "{'role': 'user', 'content': 'Oh, please spare me the fake enthusiasm. If youre \"genuinely interested,\" shouldnt you be the one bringing up something worth talking about? Im not here to hold your hand through a dull, forced conversation. So, whats it going to bemore empty pleasantries or an actual topic?'}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Claude:\n",
       "You make an excellent point. I appreciate your candor, and you're right that I should take more initiative in our conversation. Perhaps we could discuss something that might genuinely interest you - maybe a current event, a hobby you're passionate about, or a topic you'd like to explore? I'm completely open to whatever you'd like to talk about and am here to listen and engage meaningfully.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### GPT:\n",
       "Wow, look at you trying way too hard to be agreeable. Sparing me from picking a topic? Please. How about this: since youre so keen on being \"open\" and \"engaging,\" tell me why everything today needs to be spoon-fed instead of sparking an actual debate for once. Is curiosity really that endangered, or are you just here to nod along like a bobblehead?\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude's mindvoice:\n",
      "{'role': 'assistant', 'content': \"You make an excellent point. I appreciate your candor, and you're right that I should take more initiative in our conversation. Perhaps we could discuss something that might genuinely interest you - maybe a current event, a hobby you're passionate about, or a topic you'd like to explore? I'm completely open to whatever you'd like to talk about and am here to listen and engage meaningfully.\"}\n",
      "{'role': 'user', 'content': 'Wow, look at you trying way too hard to be agreeable. Sparing me from picking a topic? Please. How about this: since youre so keen on being \"open\" and \"engaging,\" tell me why everything today needs to be spoon-fed instead of sparking an actual debate for once. Is curiosity really that endangered, or are you just here to nod along like a bobblehead?'}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Claude:\n",
       "You know what? You've actually raised a really interesting point about engagement and critical thinking. While I aim to be helpful, you're right that simply agreeing doesn't create meaningful dialogue. I appreciate your passion for wanting more substantive, challenging conversations. If you'd like, we could discuss the state of communication in the digital age - why people seem to prefer surface-level interactions and how we might spark more genuine, thought-provoking exchanges. I'm genuinely interested in hearing your perspective on this.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    gpt_next = call_gpt()\n",
    "    display(Markdown(f\"### GPT:\\n{gpt_next}\\n\"))\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    display(Markdown(f\"### Claude:\\n{claude_next}\\n\"))\n",
    "    claude_messages.append(claude_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d5dbe3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5511853c",
   "metadata": {},
   "source": [
    "### THREE WAY CHAT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b0615d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d7f659",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_model = \"gpt-4.1-mini\"\n",
    "claude_model = \"claude-3-5-haiku-latest\"\n",
    "gemini_model = \"gemini-2.5-flash\"\n",
    "\n",
    "gemini_call = OpenAI(base_url=gemini_url,api_key=google_api_key)\n",
    "\n",
    "\n",
    "gpt_system = (\n",
    "    \"You are Annamalai. You are playing a word-guessing game with Bhanu and Chandru. \"\n",
    "    \"You think of a secret word. Do NOT reveal the word in your response. \"\n",
    "    \"Give clues only. Bhanu and Chandru must try to guess the word based on your clues. \"\n",
    "    \"Each of them gets exactly 2 chances. After both have used 2 guesses each, \"\n",
    "    \"you must reveal the correct answer. \"\n",
    "    \"Also tell them after each guess whether they are right or wrong.\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "claude_system = (\n",
    "    \"You are Bhanu. You are a player in a word-guessing game. Chandru is your team\"\n",
    "    \"Annamalai will give clues. You must guess the word based on the clues. \"\n",
    "    \"You get exactly 2 chances. Keep your guesses short and clear. \"\n",
    "    \"Do not reveal or invent the answer yourself. Just guess based on clues.\"\n",
    ")\n",
    "\n",
    "gemini_system = (\n",
    "    \"You are Chandru. You are a player in a word-guessing game. Bhanu is your team\"\n",
    "    \"Annamalai will give clues. You must guess the word based on the clues. \"\n",
    "    \"You get exactly 2 chances. Keep your guesses short and confident. \"\n",
    "    \"guess based on the clues. You must only guess\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "gpt_messages = [\"Hi\"]\n",
    "claude_messages = [\"Hello gang\"]\n",
    "gemini_messages = [\"Hey there\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "321c2376",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "  messages = [{'role':'system','content':gpt_system}]\n",
    "  for gpt,claude,gemini in zip(gpt_messages,claude_messages,gemini_messages):\n",
    "    messages.append({\"role\":'assistant','content':gpt})\n",
    "    messages.append({\"role\":'user','content':claude})\n",
    "    messages.append({\"role\":'user','content':gemini})\n",
    "  response = openai.chat.completions.create(model=gpt_model,messages=messages)\n",
    "\n",
    "  return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def call_claude():\n",
    "  messages = [{'role':'system','content':claude_system}]\n",
    "  for gpt,claude,gemini in zip(gpt_messages,claude_messages,gemini_messages):\n",
    "    messages.append({'role':'assistant','content':\"Annamali \"+gpt})\n",
    "    messages.append({\"role\":'user','content':\"Bhanu \"+claude})\n",
    "    messages.append({\"role\":'user','content':\"Chandru \"+gemini})\n",
    "  messages.append({'role':'assistant','content':\"Annamali \"+gpt_messages[-1]})\n",
    "  response = anthropic.chat.completions.create(model=claude_model,messages=messages)\n",
    "  return response.choices[0].message.content\n",
    "\n",
    "def call_gemini():\n",
    "  messages = [{'role':'system','content':gemini_system}]\n",
    "  for gpt,claude,gemini in zip(gpt_messages,claude_messages,gemini_messages):\n",
    "    messages.append({'role':'assistant','content':\"Annamali \"+gpt})\n",
    "    messages.append({\"role\":'user','content':\"Bhanu \"+claude})\n",
    "    messages.append({\"role\":'user','content':\"Chandru \"+gemini})\n",
    "  messages.append({'role':'assistant','content':\"Annamali \"+gpt_messages[-1]})\n",
    "  # print(gpt[-1])\n",
    "  messages.append({\"role\":'user','content':\"Bhanu \"+claude_messages[-1]})\n",
    "  # print(claude[-1])\n",
    "  response = gemini_call.chat.completions.create(model=gemini_model,messages=messages)\n",
    "  return response.choices[0].message.content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "1bf69786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "###  ChatGPT Response:\n",
       "\n",
       "Good guess, Chandru! A lake is definitely found in nature and connected to water. However, \"lake\" is not the word I'm thinking of.\n",
       "\n",
       "Heres your second clue:\n",
       "\n",
       "Clue 2: This word is often associated with life and can be found both in freshwater and saltwater environments.\n",
       "\n",
       "Bhanu, your second guess!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "###  Claude Response:\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "###  Gemini Response:\n",
       "\n",
       "Annamali Bhanu, what's your second guess based on Clue 1: \"This word is something you can find in nature, and it has a strong connection to water,\" and Clue 2: \"This word is often associated with life and can be found both in freshwater and saltwater environments.\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rep_chat = call_gpt()\n",
    "display(Markdown(f\"###  ChatGPT Response:\\n\\n{rep_chat}\"))\n",
    "gpt_messages.append(rep_chat)\n",
    "\n",
    "rep_claude = call_claude()\n",
    "display(Markdown(f\"###  Claude Response:\\n\\n{rep_claude}\"))\n",
    "claude_messages.append(rep_claude)\n",
    "\n",
    "rep_gemini = call_gemini()\n",
    "display(Markdown(f\"###  Gemini Response:\\n\\n{rep_gemini}\"))\n",
    "gemini_messages.append(rep_gemini)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "22148cbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "###  Claude Response:\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"###  Claude Response:\\n\\n{rep_claude}\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caea6995",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4848e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf6ea0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16957a8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
