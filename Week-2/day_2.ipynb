{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcb305a0",
   "metadata": {},
   "source": [
    "GRADIO Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58904cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec71395",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf082747",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "556e96b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c29152ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI()\n",
    "anthropic_url = \"https://api.anthropic.com/v1/\"\n",
    "gemini_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "anthropic = OpenAI(api_key=anthropic_api_key, base_url=anthropic_url)\n",
    "gemini = OpenAI(api_key=google_api_key, base_url=gemini_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56682ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's wrap a call to GPT-4.1-mini in a simple function\n",
    "\n",
    "system_message = \"You are a helpful assistant\"\n",
    "\n",
    "def message_gpt(prompt):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}, {\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.chat.completions.create(model=\"gpt-4.1-mini\", messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed74c019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! Large Language Models (LLMs) have become remarkably intelligent in recent years. They can understand and generate human-like text, assist with coding, debug, and even write complex pieces of software by following detailed instructions. \\n\\nRegarding creating end-to-end software:\\n\\n- **Feasibility:** LLMs can generate code for many parts of a software project, including backend, frontend, database queries, and APIs. They can produce prototypes and sometimes fully functional modules.\\n  \\n- **Limitations:** However, completely autonomous development of an entire complex software system from scratch—covering design, implementation, testing, deployment, and maintenance—is still challenging. LLMs typically require human guidance to ensure that the software meets specific requirements, is secure, efficient, and maintainable.\\n  \\n- **Current Use Cases:** Developers commonly use LLMs to accelerate coding tasks, generate boilerplate code, automate documentation, and assist with debugging. This collaboration significantly speeds up the software development lifecycle.\\n  \\nIn summary, LLMs are powerful tools that can contribute greatly to software creation and can handle many coding tasks end-to-end with guidance, but human oversight and expertise remain essential for complex, reliable software solutions.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_gpt(\"Hey How intelligent have LLMs become could they create end to end softwware\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d50267c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's a simple function\n",
    "\n",
    "def shout(text):\n",
    "    print(f\"Shout has been called with input {text}\")\n",
    "    return text.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "baed107a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shout has been called with input deshksjdcnsknvdsjkfv\n"
     ]
    }
   ],
   "source": [
    "gr.Interface(fn=shout,inputs=\"textbox\",outputs=\"textbox\",flagging_mode='never').launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc2dcb82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* Running on public URL: https://5c1964772d530aff15.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://5c1964772d530aff15.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shout has been called with input xasda\n"
     ]
    }
   ],
   "source": [
    "gr.Interface(fn=shout, inputs=\"textbox\", outputs=\"textbox\", flagging_mode=\"never\").launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "030a91c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shout has been called with input sdcsedf\n"
     ]
    }
   ],
   "source": [
    "gr.Interface(fn=shout,inputs=\"textbox\",outputs=\"textbox\",flagging_mode=\"never\").launch(inbrowser=True,auth=(\"auth\",\"122\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428d4863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7864\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shout has been called with input de\n",
      "Shout has been called with input kfrosd\n"
     ]
    }
   ],
   "source": [
    "message_input = gr.Textbox(label=\"You message: \",info=\"Enter a message to be shouted\",lines=7)\n",
    "\n",
    "message_output = gr.Textbox(label=\"Response: \",lines=8)\n",
    "\n",
    "gr.Interface(\n",
    "  fn=shout,\n",
    "  inputs = [message_input],\n",
    "  outputs=[message_output],\n",
    "  examples=[\"Hello\",\"kfrosd\"],\n",
    "  title=\"SHOUT\",\n",
    "  flagging_mode='never'\n",
    ").launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03e97558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7868\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7868/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_input = gr.Textbox(label=\"You message: \",info=\"Enter a message to be chatted\",lines=7)\n",
    "\n",
    "message_output = gr.Textbox(label=\"Response: \",lines=8)\n",
    "\n",
    "gr.Interface(\n",
    "  fn=message_gpt,\n",
    "  inputs = [message_input],\n",
    "  outputs=[message_output],\n",
    "  examples=[\"Hello\",\"kfrosd\"],\n",
    "  title=\"SHOUT\",\n",
    "  flagging_mode='never'\n",
    ").launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cbd12f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e266fe33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ba7eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a helpful assistant that responds in markdown without code blocks\"\n",
    "\n",
    "message_input = gr.Textbox(label=\"Your message:\", info=\"Enter a message for GPT-4.1-mini\", lines=7)\n",
    "message_output = gr.Markdown(label=\"Response:\")\n",
    "\n",
    "view = gr.Interface(\n",
    "    fn=message_gpt,\n",
    "    title=\"GPT\", \n",
    "    inputs=[message_input], \n",
    "    outputs=[message_output], \n",
    "    examples=[\n",
    "        \"Explain the Transformer architecture to a layperson\",\n",
    "        \"Explain the Transformer architecture to an aspiring AI engineer\",\n",
    "        ], \n",
    "    flagging_mode=\"never\"\n",
    "    )\n",
    "view.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70ccdd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_gpt(prompt):\n",
    "  message= [\n",
    "    {\"role\":\"system\",\"content\":system_message},\n",
    "    {\"role\":'user','content':prompt}\n",
    "  ]\n",
    "  stream = openai.chat.completions.create(\n",
    "    model='gpt-4.1-mini',\n",
    "    messages=message,\n",
    "    stream=True\n",
    "  )\n",
    "\n",
    "  result=\"\"\n",
    "\n",
    "  for chunk in stream:\n",
    "    result+=chunk.choices[0].delta.content or \"\"\n",
    "    yield result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5533d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages_input = gr.Textbox(label=\"Your Message:\",info=\"Enter a message for GPT-4.1-mini\",lines=7)\n",
    "message_output = gr.Markdown(label=\"Response: \")\n",
    "gr.Interface(\n",
    "  fn=stream_gpt,\n",
    "  inputs=[messages_input],\n",
    "  outputs=[message_output],\n",
    "  examples=[\"Exaplin transformers evolution is it the same as 2017 or team have modified it\"],\n",
    "  flagging_mode=\"never\"\n",
    ").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "50926806",
   "metadata": {},
   "outputs": [],
   "source": [
    "def claude_response(prompt):\n",
    "  prompt_message = [{\"role\":'user',\"content\":prompt},{\"role\":'system','content':system_message}]\n",
    "\n",
    "  stream = openai.chat.completions.create(\n",
    "    model='gpt-4.1-nano',\n",
    "    messages=prompt_message,\n",
    "    stream=True\n",
    "  )\n",
    "\n",
    "  result = \"\"\n",
    "\n",
    "  for i in stream:\n",
    "    if i is None:\n",
    "      break\n",
    "    delta = i.choices[0].delta\n",
    "    content = getattr(delta,'content',None)\n",
    "    if content:\n",
    "      result+=content\n",
    "    yield result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb1f845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\vijib\\projects\\llm_eng_manual\\.venv\\Lib\\site-packages\\gradio\\queueing.py\", line 853, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vijib\\projects\\llm_eng_manual\\.venv\\Lib\\site-packages\\gradio\\route_utils.py\", line 354, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vijib\\projects\\llm_eng_manual\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 2106, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vijib\\projects\\llm_eng_manual\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 1600, in call_function\n",
      "    prediction = await utils.async_iteration(iterator)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vijib\\projects\\llm_eng_manual\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 893, in async_iteration\n",
      "    return await anext(iterator)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vijib\\projects\\llm_eng_manual\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 884, in __anext__\n",
      "    return await anyio.to_thread.run_sync(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vijib\\projects\\llm_eng_manual\\.venv\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vijib\\projects\\llm_eng_manual\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2485, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vijib\\projects\\llm_eng_manual\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 976, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vijib\\projects\\llm_eng_manual\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 867, in run_sync_iterator_async\n",
      "    return next(iterator)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vijib\\projects\\llm_eng_manual\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 1031, in gen_wrapper\n",
      "    response = next(iterator)\n",
      "               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vijib\\AppData\\Local\\Temp\\ipykernel_368\\477760803.py\", line 13, in claude_response\n",
      "    result+=i.choices[0].delta.content\n",
      "TypeError: can only concatenate str (not \"NoneType\") to str\n"
     ]
    }
   ],
   "source": [
    "message_input = gr.Textbox(label=\"Your message: \",info=\"Enter a message for GPT-4.1-mini\",lines=7)\n",
    "\n",
    "message_output = gr.Markdown(label=\"Response: \")\n",
    "\n",
    "view = gr.Interface(\n",
    "  fn=claude_response,\n",
    "  title=\"Claude\",\n",
    "  inputs=[message_input],\n",
    "  outputs=[message_output],\n",
    "  examples=[\n",
    "    \"Exaplin transformers evolution is it the same as 2017 or team have modified it\"\n",
    "  ], flagging_mode=\"never\"\n",
    ").launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f79b083",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_model(prompt,model):\n",
    "  if model == 'GPT':\n",
    "    result = stream_gpt(prompt)\n",
    "  elif model == 'Claude':\n",
    "    result = claude_response(prompt)\n",
    "  else:\n",
    "    raise ValueError(\"Unknown model\")\n",
    "  yield from result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e517bfab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_input = gr.Textbox(label=\"Your message: \",info=\"Enter a message for the LLM\",lines=7)\n",
    "model_selector = gr.Dropdown([\"GPT\",\"Claude\"],label=\"Select Model\")\n",
    "message_output = gr.Markdown(label=\"Response: \")\n",
    "\n",
    "gr.Interface(\n",
    "  fn=stream_model,\n",
    "  title=\"LLMs\",\n",
    "  inputs=[messages_input,model_selector],\n",
    "  outputs=[message_output],\n",
    "  examples = [\n",
    "    [\"Examplain the Transformers\",\"GPT\"],\n",
    "    [\"Exaplain architecture\",\"Claude\"]\n",
    "  ],\n",
    "  flagging_mode = \"never\"\n",
    ").launch()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "939a1e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: playwright in c:\\users\\vijib\\projects\\llm_eng_manual\\.venv\\lib\\site-packages (1.56.0)\n",
      "Requirement already satisfied: pyee<14,>=13 in c:\\users\\vijib\\projects\\llm_eng_manual\\.venv\\lib\\site-packages (from playwright) (13.0.0)\n",
      "Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in c:\\users\\vijib\\projects\\llm_eng_manual\\.venv\\lib\\site-packages (from playwright) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\vijib\\projects\\llm_eng_manual\\.venv\\lib\\site-packages (from pyee<14,>=13->playwright) (4.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install playwright"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da86301b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1888031546.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mimport { chromium } from \"playwright\";\u001b[39m\n           ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import { chromium } from \"playwright\";\n",
    "def scrapper_text():\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58865632",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
