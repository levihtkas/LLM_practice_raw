{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92e8f5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import subprocess\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b3234b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key exists and begins sk-ant-\n",
      "Google API Key exists and begins AI\n",
      "Grok API Key not set (and this is optional)\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "grok_api_key = os.getenv('GROK_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set (and this is optional)\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:2]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set (and this is optional)\")\n",
    "\n",
    "if grok_api_key:\n",
    "    print(f\"Grok API Key exists and begins {grok_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"Grok API Key not set (and this is optional)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f6878b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to client libraries\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "anthropic_url = \"https://api.anthropic.com/v1/\"\n",
    "gemini_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "grok_url = \"https://api.x.ai/v1\"\n",
    "\n",
    "anthropic = OpenAI(api_key=anthropic_api_key, base_url=anthropic_url)\n",
    "gemini = OpenAI(api_key=google_api_key, base_url=gemini_url)\n",
    "grok = OpenAI(api_key=grok_api_key, base_url=grok_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9918f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_MODEL = \"gpt-5-mini\"\n",
    "CLAUDE_MODEL = \"claude-sonnet-4-5-20250929\"\n",
    "GROK_MODEL = \"grok-4\"\n",
    "GEMINI_MODEL = \"gemini-2.5-pro\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e02a3e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'os': {'system': 'Windows',\n",
       "  'arch': 'AMD64',\n",
       "  'release': '11',\n",
       "  'version': '10.0.22631',\n",
       "  'kernel': '11',\n",
       "  'distro': None,\n",
       "  'wsl': False,\n",
       "  'rosetta2_translated': False,\n",
       "  'target_triple': ''},\n",
       " 'package_managers': ['winget', 'choco'],\n",
       " 'cpu': {'brand': 'AMD Ryzen 5 5600H with Radeon Graphics',\n",
       "  'cores_logical': 12,\n",
       "  'cores_physical': 6,\n",
       "  'simd': []},\n",
       " 'toolchain': {'compilers': {'gcc': '', 'g++': '', 'clang': '', 'msvc_cl': ''},\n",
       "  'build_tools': {'cmake': '', 'ninja': '', 'make': ''},\n",
       "  'linkers': {'ld_lld': ''}}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from system_info import retrieve_system_info\n",
    "\n",
    "system_info = retrieve_system_info()\n",
    "system_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591303f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "You don’t currently have a C++ compiler installed. You’ll need to install one.\n",
       "\n",
       "Simplest setup on your Windows 11 system (recommended): Microsoft C++ Build Tools (MSVC) via winget\n",
       "\n",
       "1) Open an elevated PowerShell (Run as administrator).\n",
       "2) Install the C++ build tools (includes the Windows SDK so MSVC can build C++ out of the box):\n",
       "   winget install -e --id Microsoft.VisualStudio.2022.BuildTools --override \"--quiet --wait --norestart --nocache --add Microsoft.VisualStudio.Workload.VCTools --includeRecommended\"\n",
       "3) After installation, you can compile by first initializing the compiler environment using vcvars64.bat.\n",
       "\n",
       "Exact Python commands to compile and run main.cpp at high optimization\n",
       "\n",
       "- This uses MSVC with link-time optimization for strong runtime performance.\n",
       "\n",
       "compile_command = [\n",
       "    \"cmd\", \"/d\", \"/s\", \"/c\",\n",
       "    r'\"C:\\Program Files\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Auxiliary\\Build\\vcvars64.bat\" && cl /nologo /O2 /GL /EHsc /std:c++20 /MD main.cpp /link /LTCG /OUT:main.exe'\n",
       "]\n",
       "run_command = [\"main.exe\"]\n",
       "\n",
       "Notes:\n",
       "- vcvars64.bat sets the necessary environment variables for cl to work. The path shown is the default for VS 2022 Build Tools; if you installed to a custom location, adjust that path accordingly.\n",
       "- /O2 and /GL (with /LTCG at link) enable high optimization levels for good runtime performance. If you prefer faster compilation over maximum runtime speed, you can drop /GL and /LTCG."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "message = f\"\"\"\n",
    "Here is a report of the system information for my computer.\n",
    "I want to run a Pytorch compiler to compile a single Pytorch file called main.cpp and then execute it in the simplest way possible.\n",
    "Please reply with whether I need to install any Pytorch compiler to do this. If so, please provide the simplest step by step instructions to do so.\n",
    "\n",
    "If I'm already set up to compile Pytorch code, then I'd like to run something like this in Python to compile and execute the code:\n",
    "\n",
    "Please tell me exactly what I should use for the compile_command and run_command.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "response = openai.chat.completions.create(model=OPENAI_MODEL, messages=[{\"role\": \"user\", \"content\": message}])\n",
    "display(Markdown(response.choices[0].message.content))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95d9a4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "Your task is to convert Python code into high performance Pytorch\n",
    "Respond only with Pytorch Do not provide any explanation other than occasional comments.\n",
    "The Pytorch response needs to produce an identical output in the fastest possible time.\n",
    "\"\"\"\n",
    "\n",
    "def user_prompt_for(python):\n",
    "    return f\"\"\"\n",
    "Port this Python code to Pytorch with the fastest possible implementation that produces identical output in the least time.\n",
    "\n",
    "Respond only with Pytorch\n",
    "Python code to port:\n",
    "\n",
    "```python\n",
    "{python}\n",
    "```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04733393",
   "metadata": {},
   "outputs": [],
   "source": [
    "def messages_for(python):\n",
    "  return  [\n",
    "    {\"role\":\"system\",\"content\":system_prompt},\n",
    "    {\"role\":\"user\",\"content\":user_prompt_for(python)}\n",
    "  ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f9099753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_reply(reply):\n",
    "  with open (\"pytorch.py\",\"w\",encoding=\"utf-8\") as f:\n",
    "    f.write(reply)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4567be94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def port(client,model,python):\n",
    "  response = client.chat.completions.create(model=model,messages=messages_for(python))\n",
    "  reply = response.choices[0].message.content\n",
    "  write_reply(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d0a9e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = \"\"\"\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Data\n",
    "x = np.random.rand(1_000_000)\n",
    "w = np.random.rand(1_000_000)\n",
    "b = 0.1\n",
    "\n",
    "# Timing\n",
    "start = time.perf_counter()\n",
    "\n",
    "y = np.dot(x, w) + b\n",
    "\n",
    "end = time.perf_counter()\n",
    "\n",
    "print(\"NumPy Output:\", y)\n",
    "print(\"NumPy Time Taken:\", end - start, \"seconds\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0da047d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_python(code):\n",
    " \n",
    "    exec(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a973a36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy Output: 249929.90530039204\n",
      "NumPy Time Taken: 0.001779599999281345 seconds\n"
     ]
    }
   ],
   "source": [
    "run_python(pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3f41bbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "port(openai, OPENAI_MODEL, pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2804fa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "port(anthropic, CLAUDE_MODEL, pi)\n",
    "pt = \"\"\"\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# Data size\n",
    "n = 1_000_000\n",
    "\n",
    "# Device (use GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Use double precision to match NumPy's default dtype\n",
    "dtype = torch.float64\n",
    "\n",
    "# Generate data on chosen device\n",
    "x = torch.rand(n, dtype=dtype, device=device)\n",
    "w = torch.rand(n, dtype=dtype, device=device)\n",
    "b = 0.1\n",
    "\n",
    "# Warm-up for CUDA (if used)\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "# Timing\n",
    "start = time.perf_counter()\n",
    "y = torch.dot(x, w) + b\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.synchronize()\n",
    "end = time.perf_counter()\n",
    "\n",
    "print(\"PyTorch Output:\", y.item())\n",
    "print(\"PyTorch Time Taken:\", end - start, \"seconds\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c9804f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Output: 249771.3687180019\n",
      "PyTorch Time Taken: 0.0034960999983013608 seconds\n"
     ]
    }
   ],
   "source": [
    "run_python(pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e31e82df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy Output: 250193.68504644226\n",
      "NumPy Time Taken: 0.0030337000025610905 seconds\n"
     ]
    }
   ],
   "source": [
    "run_python(pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "154d43cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63f34eac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ff8a764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcc1dbd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vijib\\projects\\llm_eng_manual\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import gradio as gr\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3af5db36",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "openai_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "anthropic_url = \"https://api.anthropic.com/v1/\"\n",
    "gemini_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "ollama_url = \"http://localhost:11434/v1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "498af9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "anthropic = OpenAI(api_key=anthropic_api_key,base_url=anthropic_url)\n",
    "openai = OpenAI(api_key=openai_key)\n",
    "gemini = OpenAI(api_key = google_api_key,base_url=gemini_url)\n",
    "ollama = OpenAI(base_url=ollama_url,api_key=\"None\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb223a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "models= [\"gpt-5\",\"gemini-2.5-pro\",\"llama\"]\n",
    "clients = {\"gpt-5\":openai,\"gemini-2.5-pro\":gemini,\"llama\":ollama}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b3672e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "Your task is to convert Python code into high performance Pytorch\n",
    "Respond only with Pytorch Do not provide any explanation other than occasional comments.\n",
    "The Pytorch response needs to produce an identical output in the fastest possible time.\n",
    "\"\"\"\n",
    "\n",
    "def user_prompt_for(python):\n",
    "    return f\"\"\"\n",
    "Port this Python code to Pytorch with the fastest possible implementation that produces identical output in the least time.\n",
    "\n",
    "Respond only with Pytorch\n",
    "Python code to port:\n",
    "\n",
    "```python\n",
    "{python}\n",
    "```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13c06e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def messages_for(python):\n",
    "  return [{\"role\":\"user\",\"content\":user_prompt_for(python)},{\"role\":\"system\",\"content\":system_prompt}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b840b87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_output(python):\n",
    "  with open(\"output_code.py\",\"w\") as f:\n",
    "    f.write(python)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4dde1c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def port(model,python):\n",
    "  client = clients[model]\n",
    "  res = client.chat.completions.create(model=model,messages = messages_for(python))\n",
    "  reply = res.choices[0].message.content\n",
    "  write_output(reply)\n",
    "  return reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9a8dcf44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\vijib\\projects\\llm_eng_manual\\.venv\\Lib\\site-packages\\gradio\\queueing.py\", line 763, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vijib\\projects\\llm_eng_manual\\.venv\\Lib\\site-packages\\gradio\\route_utils.py\", line 354, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vijib\\projects\\llm_eng_manual\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 2106, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vijib\\projects\\llm_eng_manual\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 1588, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vijib\\projects\\llm_eng_manual\\.venv\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vijib\\projects\\llm_eng_manual\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2485, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vijib\\projects\\llm_eng_manual\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 976, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vijib\\projects\\llm_eng_manual\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 1048, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vijib\\AppData\\Local\\Temp\\ipykernel_16172\\2387510810.py\", line 2, in port\n",
      "    client = clients[model]\n",
      "             ~~~~~~~^^^^^^^\n",
      "KeyError: 'gemini-2.5-pro'\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\vijib\\projects\\llm_eng_manual\\.venv\\Lib\\site-packages\\gradio\\queueing.py\", line 763, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vijib\\projects\\llm_eng_manual\\.venv\\Lib\\site-packages\\gradio\\route_utils.py\", line 354, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vijib\\projects\\llm_eng_manual\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 2106, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vijib\\projects\\llm_eng_manual\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 1588, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vijib\\projects\\llm_eng_manual\\.venv\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vijib\\projects\\llm_eng_manual\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2485, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vijib\\projects\\llm_eng_manual\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 976, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vijib\\projects\\llm_eng_manual\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 1048, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vijib\\AppData\\Local\\Temp\\ipykernel_16172\\2387510810.py\", line 2, in port\n",
      "    client = clients[model]\n",
      "             ~~~~~~~^^^^^^^\n",
      "KeyError: 'gemini-2.5-pro'\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\vijib\\projects\\llm_eng_manual\\.venv\\Lib\\site-packages\\gradio\\queueing.py\", line 763, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vijib\\projects\\llm_eng_manual\\.venv\\Lib\\site-packages\\gradio\\route_utils.py\", line 354, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vijib\\projects\\llm_eng_manual\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 2106, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vijib\\projects\\llm_eng_manual\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 1588, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vijib\\projects\\llm_eng_manual\\.venv\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vijib\\projects\\llm_eng_manual\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2485, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vijib\\projects\\llm_eng_manual\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 976, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vijib\\projects\\llm_eng_manual\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 1048, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vijib\\AppData\\Local\\Temp\\ipykernel_16172\\2387510810.py\", line 2, in port\n",
      "    client = clients[model]\n",
      "             ~~~~~~~^^^^^^^\n",
      "KeyError: 'gemini-2.5-pro'\n"
     ]
    }
   ],
   "source": [
    "with gr.Blocks() as ui:\n",
    "  with gr.Row():\n",
    "    python = gr.Textbox(label=\"Python code: \",lines=28,value=pi)\n",
    "    pytorch = gr.Textbox(label=\"Pytorch code: \",lines=28)\n",
    "  with gr.Row():\n",
    "    model = gr.Dropdown(models,label=\"Select model\",value=models[0])\n",
    "    convert=gr.Button(\"Convert Code\")\n",
    "  \n",
    "  convert.click(port,inputs=[model,python],outputs = [pytorch])\n",
    "ui.launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf31a9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Output: 249780.3131405094\n",
      "PyTorch Time Taken: 0.030948400002671406 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "# Select fastest device available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = torch.float64  # match NumPy default precision\n",
    "\n",
    "# Data\n",
    "x = torch.rand(1_000_000, dtype=dtype, device=device)\n",
    "w = torch.rand(1_000_000, dtype=dtype, device=device)\n",
    "b = torch.tensor(0.1, dtype=dtype, device=device)\n",
    "\n",
    "# Timing\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.synchronize()\n",
    "start = time.perf_counter()\n",
    "\n",
    "y = torch.dot(x, w) + b\n",
    "\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.synchronize()\n",
    "end = time.perf_counter()\n",
    "\n",
    "print(\"PyTorch Output:\", y.item())\n",
    "print(\"PyTorch Time Taken:\", end - start, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f346a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cc25d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
