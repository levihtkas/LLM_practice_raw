{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e8f5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import subprocess\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3234b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "grok_api_key = os.getenv('GROK_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set (and this is optional)\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:2]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set (and this is optional)\")\n",
    "\n",
    "if grok_api_key:\n",
    "    print(f\"Grok API Key exists and begins {grok_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"Grok API Key not set (and this is optional)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6878b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to client libraries\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "anthropic_url = \"https://api.anthropic.com/v1/\"\n",
    "gemini_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "grok_url = \"https://api.x.ai/v1\"\n",
    "\n",
    "anthropic = OpenAI(api_key=anthropic_api_key, base_url=anthropic_url)\n",
    "gemini = OpenAI(api_key=google_api_key, base_url=gemini_url)\n",
    "grok = OpenAI(api_key=grok_api_key, base_url=grok_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9918f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_MODEL = \"gpt-5-mini\"\n",
    "CLAUDE_MODEL = \"claude-sonnet-4-5-20250929\"\n",
    "GROK_MODEL = \"grok-4\"\n",
    "GEMINI_MODEL = \"gemini-2.5-pro\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e02a3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from system_info import retrieve_system_info\n",
    "\n",
    "system_info = retrieve_system_info()\n",
    "system_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591303f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = f\"\"\"\n",
    "Here is a report of the system information for my computer.\n",
    "I want to run a Pytorch compiler to compile a single Pytorch file called main.cpp and then execute it in the simplest way possible.\n",
    "Please reply with whether I need to install any Pytorch compiler to do this. If so, please provide the simplest step by step instructions to do so.\n",
    "\n",
    "If I'm already set up to compile Pytorch code, then I'd like to run something like this in Python to compile and execute the code:\n",
    "\n",
    "Please tell me exactly what I should use for the compile_command and run_command.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "response = openai.chat.completions.create(model=OPENAI_MODEL, messages=[{\"role\": \"user\", \"content\": message}])\n",
    "display(Markdown(response.choices[0].message.content))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d9a4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "Your task is to convert Python code into high performance Pytorch\n",
    "Respond only with Pytorch Do not provide any explanation other than occasional comments.\n",
    "The Pytorch response needs to produce an identical output in the fastest possible time.\n",
    "\"\"\"\n",
    "\n",
    "def user_prompt_for(python):\n",
    "    return f\"\"\"\n",
    "Port this Python code to Pytorch with the fastest possible implementation that produces identical output in the least time.\n",
    "\n",
    "Respond only with Pytorch\n",
    "Python code to port:\n",
    "\n",
    "```python\n",
    "{python}\n",
    "```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04733393",
   "metadata": {},
   "outputs": [],
   "source": [
    "def messages_for(python):\n",
    "  return  [\n",
    "    {\"role\":\"system\",\"content\":system_prompt},\n",
    "    {\"role\":\"user\",\"content\":user_prompt_for(python)}\n",
    "  ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9099753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_reply(reply):\n",
    "  with open (\"pytorch.py\",\"w\",encoding=\"utf-8\") as f:\n",
    "    f.write(reply)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4567be94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def port(client,model,python):\n",
    "  response = client.chat.completions.create(model=model,messages=messages_for(python))\n",
    "  reply = response.choices[0].message.content\n",
    "  write_reply(reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0a9e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = \"\"\"\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Data\n",
    "x = np.random.rand(1_000_000)\n",
    "w = np.random.rand(1_000_000)\n",
    "b = 0.1\n",
    "\n",
    "# Timing\n",
    "start = time.perf_counter()\n",
    "\n",
    "y = np.dot(x, w) + b\n",
    "\n",
    "end = time.perf_counter()\n",
    "\n",
    "print(\"NumPy Output:\", y)\n",
    "print(\"NumPy Time Taken:\", end - start, \"seconds\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da047d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_python(code):\n",
    " \n",
    "    exec(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a973a36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_python(pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f41bbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "port(openai, OPENAI_MODEL, pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2804fa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "port(anthropic, CLAUDE_MODEL, pi)\n",
    "pt = \"\"\"\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# Data size\n",
    "n = 1_000_000\n",
    "\n",
    "# Device (use GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Use double precision to match NumPy's default dtype\n",
    "dtype = torch.float64\n",
    "\n",
    "# Generate data on chosen device\n",
    "x = torch.rand(n, dtype=dtype, device=device)\n",
    "w = torch.rand(n, dtype=dtype, device=device)\n",
    "b = 0.1\n",
    "\n",
    "# Warm-up for CUDA (if used)\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "# Timing\n",
    "start = time.perf_counter()\n",
    "y = torch.dot(x, w) + b\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.synchronize()\n",
    "end = time.perf_counter()\n",
    "\n",
    "print(\"PyTorch Output:\", y.item())\n",
    "print(\"PyTorch Time Taken:\", end - start, \"seconds\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9804f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_python(pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31e82df",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_python(pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154d43cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f34eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff8a764",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc1dbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gradio as gr\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af5db36",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "openai_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "anthropic_url = \"https://api.anthropic.com/v1/\"\n",
    "gemini_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "ollama_url = \"http://localhost:11434/v1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498af9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "anthropic = OpenAI(api_key=anthropic_api_key,base_url=anthropic_url)\n",
    "openai = OpenAI(api_key=openai_key)\n",
    "gemini = OpenAI(api_key = google_api_key,base_url=gemini_url)\n",
    "ollama = OpenAI(base_url=ollama_url,api_key=\"None\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb223a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "models= [\"gpt-5\",\"gemini-2.5-pro\",\"llama\"]\n",
    "clients = {\"gpt-5\":openai,\"gemini-2.5-pro\":gemini,\"llama\":ollama}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3672e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "Your task is to convert Python code into high performance Pytorch\n",
    "Respond only with Pytorch Do not provide any explanation other than occasional comments.\n",
    "The Pytorch response needs to produce an identical output in the fastest possible time.\n",
    "\"\"\"\n",
    "\n",
    "def user_prompt_for(python):\n",
    "    return f\"\"\"\n",
    "Port this Python code to Pytorch with the fastest possible implementation that produces identical output in the least time.\n",
    "\n",
    "Respond only with Pytorch\n",
    "Python code to port:\n",
    "\n",
    "```python\n",
    "{python}\n",
    "```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c06e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def messages_for(python):\n",
    "  return [{\"role\":\"user\",\"content\":user_prompt_for(python)},{\"role\":\"system\",\"content\":system_prompt}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b840b87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_output(python):\n",
    "  with open(\"output_code.py\",\"w\") as f:\n",
    "    f.write(python)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dde1c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def port(model,python):\n",
    "  client = clients[model]\n",
    "  res = client.chat.completions.create(model=model,messages = messages_for(python))\n",
    "  reply = res.choices[0].message.content\n",
    "  write_output(reply)\n",
    "  return reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8dcf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks() as ui:\n",
    "  with gr.Row():\n",
    "    python = gr.Textbox(label=\"Python code: \",lines=28,value=pi)\n",
    "    pytorch = gr.Textbox(label=\"Pytorch code: \",lines=28)\n",
    "  with gr.Row():\n",
    "    model = gr.Dropdown(models,label=\"Select model\",value=models[0])\n",
    "    convert=gr.Button(\"Convert Code\")\n",
    "  \n",
    "  convert.click(port,inputs=[model,python],outputs = [pytorch])\n",
    "ui.launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf31a9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "# Select fastest device available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = torch.float64  # match NumPy default precision\n",
    "\n",
    "# Data\n",
    "x = torch.rand(1_000_000, dtype=dtype, device=device)\n",
    "w = torch.rand(1_000_000, dtype=dtype, device=device)\n",
    "b = torch.tensor(0.1, dtype=dtype, device=device)\n",
    "\n",
    "# Timing\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.synchronize()\n",
    "start = time.perf_counter()\n",
    "\n",
    "y = torch.dot(x, w) + b\n",
    "\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.synchronize()\n",
    "end = time.perf_counter()\n",
    "\n",
    "print(\"PyTorch Output:\", y.item())\n",
    "print(\"PyTorch Time Taken:\", end - start, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f346a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cc25d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
